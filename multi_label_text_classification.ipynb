{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DelaramRajaei/multi_label_text_classification/blob/main/multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label text classification"
      ],
      "metadata": {
        "id": "cZG_t10KB-OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Dataset size is not enough to train the model. If we add more layers, it will overfit and on the other hand, if we use more dropout or regularization(methods to overcome overfitting) the model accuracy will we low and arround 50%.\n",
        "However I manged to tackle the issue of overfitting and increase the accuracy up to 70%."
      ],
      "metadata": {
        "id": "RhYwOshpQAmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fhOSK8R0AKv",
        "outputId": "a568bda6-10ea-492e-ff8f-6336ba17d231"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 22 08:23:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    31W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information"
      ],
      "metadata": {
        "id": "3vEj1-aNCAEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Author:** [Farrokh Karimi](https://farrokhkarimi.github.io/)  \n",
        "**Edited By:** [Delaram Rajaei](https://delaramrajaei.github.io/)  \n",
        "**Description:** In this notebook, we want to classify the Ronash dataset into 20 category."
      ],
      "metadata": {
        "id": "Bij91pNWQOR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edits can be found by \"# Edited\" tags."
      ],
      "metadata": {
        "id": "vWIRRneJ08Kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "1BgIQvHsCDTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "sL739kMAEsU9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "xnPPRyMnCF6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset"
      ],
      "metadata": {
        "id": "IFqRQPb3CSTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading Data from the Google Drive link\n",
        "!gdown 1Yq3XTnACkvaIiNlhX09Zth55nOau7jQy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHa9m3aLEbyP",
        "outputId": "88cf1b1f-23fd-4c95-ca57-39a2849eee89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yq3XTnACkvaIiNlhX09Zth55nOau7jQy\n",
            "To: /content/Ronash_DS_Assignment.csv\n",
            "\r  0% 0.00/1.05M [00:00<?, ?B/s]\r100% 1.05M/1.05M [00:00<00:00, 144MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QpBuoWrC5QS",
        "outputId": "cba43677-9b8e-43d6-d28f-1b5b7f46f629"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.01-1.56.h5   model.17-2.67.h5   model.42-1.31.h5\n",
            "model.01-15.88.h5  model.17-2.77.h5   model.42-1.39.h5\n",
            "model.01-16.36.h5  model.17-2.89.h5   model.42-1.55.h5\n",
            "model.01-16.43.h5  model.173-1.01.h5  model.42-1.56.h5\n",
            "model.01-1.82.h5   model.173-1.15.h5  model.42-1.61.h5\n",
            "model.01-1.91.h5   model.173-1.20.h5  model.42-1.68.h5\n",
            "model.01-2.63.h5   model.174-1.01.h5  model.42-1.99.h5\n",
            "model.01-2.64.h5   model.174-1.15.h5  model.42-2.05.h5\n",
            "model.01-2.66.h5   model.174-1.20.h5  model.42-2.32.h5\n",
            "model.01-2.67.h5   model.175-1.01.h5  model.42-2.61.h5\n",
            "model.01-2.68.h5   model.175-1.15.h5  model.42-2.73.h5\n",
            "model.01-2.70.h5   model.175-1.20.h5  model.43-1.16.h5\n",
            "model.01-2.76.h5   model.176-1.00.h5  model.43-1.18.h5\n",
            "model.01-2.78.h5   model.176-1.15.h5  model.43-1.26.h5\n",
            "model.01-2.80.h5   model.176-1.19.h5  model.43-1.27.h5\n",
            "model.01-2.81.h5   model.177-1.00.h5  model.43-1.32.h5\n",
            "model.01-2.82.h5   model.177-1.14.h5  model.43-1.42.h5\n",
            "model.01-2.83.h5   model.177-1.19.h5  model.43-1.53.h5\n",
            "model.01-2.86.h5   model.178-1.00.h5  model.43-1.55.h5\n",
            "model.01-2.87.h5   model.178-1.15.h5  model.43-1.58.h5\n",
            "model.01-2.88.h5   model.178-1.19.h5  model.43-1.59.h5\n",
            "model.01-2.94.h5   model.179-1.00.h5  model.43-1.67.h5\n",
            "model.01-4.38.h5   model.179-1.15.h5  model.43-1.97.h5\n",
            "model.02-1.60.h5   model.179-1.19.h5  model.43-2.01.h5\n",
            "model.02-1.82.h5   model.180-1.00.h5  model.43-2.29.h5\n",
            "model.02-1.89.h5   model.180-1.15.h5  model.43-2.61.h5\n",
            "model.02-2.61.h5   model.180-1.19.h5  model.43-2.73.h5\n",
            "model.02-2.62.h5   model.181-1.00.h5  model.44-1.17.h5\n",
            "model.02-2.63.h5   model.181-1.15.h5  model.44-1.25.h5\n",
            "model.02-2.64.h5   model.181-1.19.h5  model.44-1.26.h5\n",
            "model.02-2.65.h5   model.18-1.54.h5   model.44-1.31.h5\n",
            "model.02-2.69.h5   model.18-1.67.h5   model.44-1.39.h5\n",
            "model.02-2.72.h5   model.18-1.70.h5   model.44-1.52.h5\n",
            "model.02-2.74.h5   model.18-1.72.h5   model.44-1.54.h5\n",
            "model.02-2.80.h5   model.18-1.74.h5   model.44-1.57.h5\n",
            "model.02-2.82.h5   model.18-1.78.h5   model.44-1.66.h5\n",
            "model.02-2.85.h5   model.18-1.79.h5   model.44-1.96.h5\n",
            "model.02-2.87.h5   model.18-1.85.h5   model.44-1.98.h5\n",
            "model.02-2.94.h5   model.18-1.86.h5   model.44-2.26.h5\n",
            "model.02-4.14.h5   model.18-1.92.h5   model.44-2.61.h5\n",
            "model.02-7.82.h5   model.18-1.96.h5   model.44-2.72.h5\n",
            "model.02-9.10.h5   model.18-2.05.h5   model.45-1.18.h5\n",
            "model.02-9.21.h5   model.182-1.00.h5  model.45-1.23.h5\n",
            "model.03-1.62.h5   model.182-1.14.h5  model.45-1.28.h5\n",
            "model.03-1.83.h5   model.182-1.19.h5  model.45-1.31.h5\n",
            "model.03-1.87.h5   model.18-2.16.h5   model.45-1.47.h5\n",
            "model.03-2.56.h5   model.18-2.19.h5   model.45-1.51.h5\n",
            "model.03-2.60.h5   model.18-2.22.h5   model.45-1.53.h5\n",
            "model.03-2.61.h5   model.18-2.25.h5   model.45-1.55.h5\n",
            "model.03-2.62.h5   model.18-2.28.h5   model.45-1.65.h5\n",
            "model.03-2.63.h5   model.18-2.61.h5   model.45-1.94.h5\n",
            "model.03-2.64.h5   model.18-2.63.h5   model.45-1.95.h5\n",
            "model.03-2.66.h5   model.18-2.67.h5   model.45-2.22.h5\n",
            "model.03-2.69.h5   model.18-2.76.h5   model.45-2.61.h5\n",
            "model.03-2.71.h5   model.18-2.86.h5   model.45-2.72.h5\n",
            "model.03-2.78.h5   model.183-1.00.h5  model.46-1.15.h5\n",
            "model.03-2.79.h5   model.183-1.14.h5  model.46-1.16.h5\n",
            "model.03-2.81.h5   model.183-1.19.h5  model.46-1.22.h5\n",
            "model.03-2.84.h5   model.184-1.00.h5  model.46-1.26.h5\n",
            "model.03-2.85.h5   model.184-1.15.h5  model.46-1.32.h5\n",
            "model.03-2.86.h5   model.184-1.19.h5  model.46-1.40.h5\n",
            "model.03-2.93.h5   model.185-1.00.h5  model.46-1.49.h5\n",
            "model.03-3.58.h5   model.185-1.14.h5  model.46-1.52.h5\n",
            "model.03-3.96.h5   model.185-1.19.h5  model.46-1.54.h5\n",
            "model.03-4.58.h5   model.186-1.00.h5  model.46-1.64.h5\n",
            "model.03-4.71.h5   model.186-1.14.h5  model.46-1.92.h5\n",
            "model.04-1.66.h5   model.186-1.19.h5  model.46-1.93.h5\n",
            "model.04-1.82.h5   model.187-1.00.h5  model.46-2.18.h5\n",
            "model.04-1.86.h5   model.187-1.14.h5  model.46-2.61.h5\n",
            "model.04-2.52.h5   model.187-1.19.h5  model.46-2.72.h5\n",
            "model.04-2.60.h5   model.188-0.99.h5  model.47-1.14.h5\n",
            "model.04-2.61.h5   model.188-1.15.h5  model.47-1.21.h5\n",
            "model.04-2.62.h5   model.188-1.19.h5  model.47-1.26.h5\n",
            "model.04-2.63.h5   model.189-0.99.h5  model.47-1.29.h5\n",
            "model.04-2.64.h5   model.189-1.14.h5  model.47-1.35.h5\n",
            "model.04-2.68.h5   model.189-1.18.h5  model.47-1.48.h5\n",
            "model.04-2.69.h5   model.190-0.99.h5  model.47-1.51.h5\n",
            "model.04-2.71.h5   model.190-1.14.h5  model.47-1.53.h5\n",
            "model.04-2.77.h5   model.190-1.18.h5  model.47-1.55.h5\n",
            "model.04-2.78.h5   model.191-0.99.h5  model.47-1.63.h5\n",
            "model.04-2.80.h5   model.191-1.14.h5  model.47-1.90.h5\n",
            "model.04-2.81.h5   model.191-1.18.h5  model.47-1.91.h5\n",
            "model.04-2.83.h5   model.19-1.47.h5   model.47-2.15.h5\n",
            "model.04-2.84.h5   model.19-1.61.h5   model.47-2.61.h5\n",
            "model.04-2.85.h5   model.19-1.66.h5   model.47-2.72.h5\n",
            "model.04-2.91.h5   model.19-1.67.h5   model.48-1.14.h5\n",
            "model.04-2.92.h5   model.19-1.72.h5   model.48-1.20.h5\n",
            "model.04-3.81.h5   model.19-1.74.h5   model.48-1.24.h5\n",
            "model.05-1.66.h5   model.19-1.76.h5   model.48-1.29.h5\n",
            "model.05-1.83.h5   model.19-1.80.h5   model.48-1.37.h5\n",
            "model.05-1.84.h5   model.19-1.81.h5   model.48-1.46.h5\n",
            "model.05-2.46.h5   model.19-1.82.h5   model.48-1.49.h5\n",
            "model.05-2.58.h5   model.19-1.88.h5   model.48-1.51.h5\n",
            "model.05-2.60.h5   model.19-1.89.h5   model.48-1.53.h5\n",
            "model.05-2.61.h5   model.19-1.91.h5   model.48-1.62.h5\n",
            "model.05-2.62.h5   model.19-2.01.h5   model.48-1.87.h5\n",
            "model.05-2.63.h5   model.192-0.99.h5  model.48-1.90.h5\n",
            "model.05-2.64.h5   model.19-2.10.h5   model.48-2.12.h5\n",
            "model.05-2.67.h5   model.192-1.14.h5  model.48-2.61.h5\n",
            "model.05-2.68.h5   model.192-1.18.h5  model.48-2.72.h5\n",
            "model.05-2.69.h5   model.19-2.16.h5   model.49-1.12.h5\n",
            "model.05-2.70.h5   model.19-2.17.h5   model.49-1.14.h5\n",
            "model.05-2.76.h5   model.19-2.21.h5   model.49-1.19.h5\n",
            "model.05-2.79.h5   model.19-2.24.h5   model.49-1.24.h5\n",
            "model.05-2.80.h5   model.19-2.61.h5   model.49-1.28.h5\n",
            "model.05-2.81.h5   model.19-2.62.h5   model.49-1.37.h5\n",
            "model.05-2.82.h5   model.19-2.63.h5   model.49-1.45.h5\n",
            "model.05-2.83.h5   model.19-2.67.h5   model.49-1.48.h5\n",
            "model.05-2.88.h5   model.19-2.77.h5   model.49-1.50.h5\n",
            "model.05-3.68.h5   model.19-2.83.h5   model.49-1.55.h5\n",
            "model.06-1.67.h5   model.193-0.99.h5  model.49-1.61.h5\n",
            "model.06-1.83.h5   model.193-1.14.h5  model.49-1.85.h5\n",
            "model.06-1.86.h5   model.193-1.18.h5  model.49-1.88.h5\n",
            "model.06-2.38.h5   model.194-0.99.h5  model.49-2.09.h5\n",
            "model.06-2.55.h5   model.194-1.15.h5  model.49-2.72.h5\n",
            "model.06-2.59.h5   model.194-1.18.h5  model.50-1.12.h5\n",
            "model.06-2.60.h5   model.195-0.99.h5  model.50-1.15.h5\n",
            "model.06-2.61.h5   model.195-1.14.h5  model.50-1.18.h5\n",
            "model.06-2.62.h5   model.195-1.18.h5  model.50-1.23.h5\n",
            "model.06-2.63.h5   model.196-0.99.h5  model.50-1.33.h5\n",
            "model.06-2.66.h5   model.196-1.14.h5  model.50-1.41.h5\n",
            "model.06-2.67.h5   model.196-1.18.h5  model.50-1.44.h5\n",
            "model.06-2.68.h5   model.197-0.99.h5  model.50-1.48.h5\n",
            "model.06-2.70.h5   model.197-1.14.h5  model.50-1.49.h5\n",
            "model.06-2.75.h5   model.197-1.18.h5  model.50-1.54.h5\n",
            "model.06-2.77.h5   model.198-0.99.h5  model.50-1.60.h5\n",
            "model.06-2.78.h5   model.198-1.14.h5  model.50-1.83.h5\n",
            "model.06-2.79.h5   model.198-1.18.h5  model.50-1.87.h5\n",
            "model.06-2.81.h5   model.199-0.99.h5  model.50-2.07.h5\n",
            "model.06-2.83.h5   model.199-1.13.h5  model.50-2.72.h5\n",
            "model.06-3.56.h5   model.199-1.18.h5  model.51-1.11.h5\n",
            "model.07-1.70.h5   model.200-0.99.h5  model.51-1.12.h5\n",
            "model.07-1.83.h5   model.200-1.13.h5  model.51-1.17.h5\n",
            "model.07-2.29.h5   model.200-1.18.h5  model.51-1.25.h5\n",
            "model.07-2.44.h5   model.20-1.40.h5   model.51-1.30.h5\n",
            "model.07-2.58.h5   model.20-1.56.h5   model.51-1.43.h5\n",
            "model.07-2.60.h5   model.20-1.62.h5   model.51-1.44.h5\n",
            "model.07-2.61.h5   model.20-1.63.h5   model.51-1.47.h5\n",
            "model.07-2.62.h5   model.20-1.70.h5   model.51-1.52.h5\n",
            "model.07-2.63.h5   model.20-1.71.h5   model.51-1.58.h5\n",
            "model.07-2.66.h5   model.20-1.73.h5   model.51-1.81.h5\n",
            "model.07-2.67.h5   model.20-1.76.h5   model.51-1.86.h5\n",
            "model.07-2.68.h5   model.20-1.77.h5   model.51-2.05.h5\n",
            "model.07-2.69.h5   model.20-1.84.h5   model.51-2.71.h5\n",
            "model.07-2.70.h5   model.20-1.87.h5   model.52-1.09.h5\n",
            "model.07-2.71.h5   model.20-1.99.h5   model.52-1.10.h5\n",
            "model.07-2.72.h5   model.20-2.04.h5   model.52-1.16.h5\n",
            "model.07-2.73.h5   model.20-2.12.h5   model.52-1.22.h5\n",
            "model.07-2.76.h5   model.20-2.13.h5   model.52-1.31.h5\n",
            "model.07-2.77.h5   model.20-2.17.h5   model.52-1.42.h5\n",
            "model.07-2.78.h5   model.20-2.22.h5   model.52-1.46.h5\n",
            "model.07-2.80.h5   model.20-2.61.h5   model.52-1.52.h5\n",
            "model.07-3.46.h5   model.20-2.62.h5   model.52-1.57.h5\n",
            "model.08-1.73.h5   model.20-2.67.h5   model.52-1.79.h5\n",
            "model.08-1.81.h5   model.20-2.76.h5   model.52-1.84.h5\n",
            "model.08-1.84.h5   model.20-2.81.h5   model.52-2.03.h5\n",
            "model.08-2.21.h5   model.21-1.37.h5   model.52-2.71.h5\n",
            "model.08-2.28.h5   model.21-1.51.h5   model.53-1.10.h5\n",
            "model.08-2.54.h5   model.21-1.58.h5   model.53-1.15.h5\n",
            "model.08-2.55.h5   model.21-1.68.h5   model.53-1.22.h5\n",
            "model.08-2.56.h5   model.21-1.70.h5   model.53-1.30.h5\n",
            "model.08-2.59.h5   model.21-1.72.h5   model.53-1.41.h5\n",
            "model.08-2.60.h5   model.21-1.80.h5   model.53-1.44.h5\n",
            "model.08-2.61.h5   model.21-1.82.h5   model.53-1.46.h5\n",
            "model.08-2.62.h5   model.21-1.95.h5   model.53-1.51.h5\n",
            "model.08-2.65.h5   model.21-1.98.h5   model.53-1.56.h5\n",
            "model.08-2.66.h5   model.21-2.09.h5   model.53-1.78.h5\n",
            "model.08-2.67.h5   model.21-2.14.h5   model.53-1.83.h5\n",
            "model.08-2.69.h5   model.21-2.19.h5   model.53-2.01.h5\n",
            "model.08-2.72.h5   model.21-2.61.h5   model.53-2.71.h5\n",
            "model.08-2.74.h5   model.21-2.62.h5   model.54-1.09.h5\n",
            "model.08-2.77.h5   model.21-2.67.h5   model.54-1.14.h5\n",
            "model.08-2.80.h5   model.21-2.76.h5   model.54-1.21.h5\n",
            "model.08-3.37.h5   model.21-2.79.h5   model.54-1.31.h5\n",
            "model.09-1.76.h5   model.22-1.35.h5   model.54-1.39.h5\n",
            "model.09-1.81.h5   model.22-1.46.h5   model.54-1.43.h5\n",
            "model.09-1.86.h5   model.22-1.54.h5   model.54-1.45.h5\n",
            "model.09-2.13.h5   model.22-1.65.h5   model.54-1.53.h5\n",
            "model.09-2.44.h5   model.22-1.66.h5   model.54-1.56.h5\n",
            "model.09-2.46.h5   model.22-1.68.h5   model.54-1.76.h5\n",
            "model.09-2.48.h5   model.22-1.69.h5   model.54-1.82.h5\n",
            "model.09-2.52.h5   model.22-1.73.h5   model.54-1.99.h5\n",
            "model.09-2.56.h5   model.22-1.74.h5   model.54-2.71.h5\n",
            "model.09-2.57.h5   model.22-1.79.h5   model.55-1.09.h5\n",
            "model.09-2.58.h5   model.22-1.92.h5   model.55-1.13.h5\n",
            "model.09-2.59.h5   model.22-1.93.h5   model.55-1.22.h5\n",
            "model.09-2.61.h5   model.22-2.05.h5   model.55-1.34.h5\n",
            "model.09-2.62.h5   model.22-2.06.h5   model.55-1.38.h5\n",
            "model.09-2.63.h5   model.22-2.10.h5   model.55-1.40.h5\n",
            "model.09-2.65.h5   model.22-2.19.h5   model.55-1.42.h5\n",
            "model.09-2.67.h5   model.22-2.60.h5   model.55-1.44.h5\n",
            "model.09-2.68.h5   model.22-2.61.h5   model.55-1.52.h5\n",
            "model.09-2.70.h5   model.22-2.67.h5   model.55-1.55.h5\n",
            "model.09-2.74.h5   model.22-2.76.h5   model.55-1.75.h5\n",
            "model.09-2.79.h5   model.22-2.77.h5   model.55-1.81.h5\n",
            "model.09-3.29.h5   model.23-1.32.h5   model.55-1.98.h5\n",
            "model.100-0.96.h5  model.23-1.41.h5   model.55-2.71.h5\n",
            "model.100-1.12.h5  model.23-1.49.h5   model.56-1.08.h5\n",
            "model.100-1.26.h5  model.23-1.62.h5   model.56-1.13.h5\n",
            "model.100-1.32.h5  model.23-1.63.h5   model.56-1.31.h5\n",
            "model.100-1.38.h5  model.23-1.64.h5   model.56-1.38.h5\n",
            "model.100-2.68.h5  model.23-1.68.h5   model.56-1.41.h5\n",
            "model.101-1.12.h5  model.23-1.69.h5   model.56-1.43.h5\n",
            "model.101-1.25.h5  model.23-1.71.h5   model.56-1.45.h5\n",
            "model.101-1.32.h5  model.23-1.74.h5   model.56-1.52.h5\n",
            "model.101-1.40.h5  model.23-1.87.h5   model.56-1.55.h5\n",
            "model.101-2.68.h5  model.23-1.90.h5   model.56-1.74.h5\n",
            "model.10-1.78.h5   model.23-2.01.h5   model.56-1.80.h5\n",
            "model.10-1.79.h5   model.23-2.04.h5   model.56-1.96.h5\n",
            "model.10-1.85.h5   model.23-2.07.h5   model.56-2.71.h5\n",
            "model.10-2.02.h5   model.23-2.16.h5   model.57-1.08.h5\n",
            "model.10-2.07.h5   model.23-2.59.h5   model.57-1.12.h5\n",
            "model.102-1.11.h5  model.23-2.60.h5   model.57-1.32.h5\n",
            "model.102-1.25.h5  model.23-2.61.h5   model.57-1.37.h5\n",
            "model.102-1.31.h5  model.23-2.67.h5   model.57-1.40.h5\n",
            "model.102-1.39.h5  model.23-2.75.h5   model.57-1.42.h5\n",
            "model.102-2.68.h5  model.23-2.76.h5   model.57-1.47.h5\n",
            "model.10-2.32.h5   model.24-1.32.h5   model.57-1.51.h5\n",
            "model.10-2.33.h5   model.24-1.35.h5   model.57-1.53.h5\n",
            "model.10-2.40.h5   model.24-1.47.h5   model.57-1.73.h5\n",
            "model.10-2.44.h5   model.24-1.58.h5   model.57-1.78.h5\n",
            "model.10-2.48.h5   model.24-1.59.h5   model.57-1.95.h5\n",
            "model.10-2.50.h5   model.24-1.60.h5   model.57-2.71.h5\n",
            "model.10-2.53.h5   model.24-1.62.h5   model.58-1.08.h5\n",
            "model.10-2.54.h5   model.24-1.64.h5   model.58-1.11.h5\n",
            "model.10-2.55.h5   model.24-1.66.h5   model.58-1.31.h5\n",
            "model.10-2.60.h5   model.24-1.70.h5   model.58-1.36.h5\n",
            "model.10-2.61.h5   model.24-1.83.h5   model.58-1.39.h5\n",
            "model.10-2.62.h5   model.24-1.87.h5   model.58-1.41.h5\n",
            "model.10-2.65.h5   model.24-1.97.h5   model.58-1.49.h5\n",
            "model.10-2.67.h5   model.24-2.01.h5   model.58-1.52.h5\n",
            "model.10-2.71.h5   model.24-2.04.h5   model.58-1.72.h5\n",
            "model.10-2.79.h5   model.24-2.12.h5   model.58-1.76.h5\n",
            "model.103-1.11.h5  model.24-2.57.h5   model.58-1.94.h5\n",
            "model.103-1.24.h5  model.24-2.58.h5   model.58-2.71.h5\n",
            "model.103-1.31.h5  model.24-2.61.h5   model.59-1.08.h5\n",
            "model.103-1.41.h5  model.24-2.67.h5   model.59-1.10.h5\n",
            "model.10-3.21.h5   model.24-2.74.h5   model.59-1.31.h5\n",
            "model.104-1.11.h5  model.24-2.75.h5   model.59-1.36.h5\n",
            "model.104-1.24.h5  model.25-1.30.h5   model.59-1.38.h5\n",
            "model.104-1.31.h5  model.25-1.32.h5   model.59-1.41.h5\n",
            "model.104-1.40.h5  model.25-1.42.h5   model.59-1.50.h5\n",
            "model.105-1.11.h5  model.25-1.45.h5   model.59-1.51.h5\n",
            "model.105-1.24.h5  model.25-1.55.h5   model.59-1.71.h5\n",
            "model.105-1.30.h5  model.25-1.56.h5   model.59-1.75.h5\n",
            "model.105-1.42.h5  model.25-1.57.h5   model.59-1.93.h5\n",
            "model.106-1.11.h5  model.25-1.58.h5   model.59-2.71.h5\n",
            "model.106-1.24.h5  model.25-1.60.h5   model.60-1.06.h5\n",
            "model.106-1.30.h5  model.25-1.61.h5   model.60-1.09.h5\n",
            "model.107-1.10.h5  model.25-1.62.h5   model.60-1.34.h5\n",
            "model.107-1.24.h5  model.25-1.67.h5   model.60-1.38.h5\n",
            "model.107-1.29.h5  model.25-1.71.h5   model.60-1.40.h5\n",
            "model.108-1.10.h5  model.25-1.79.h5   model.60-1.50.h5\n",
            "model.108-1.24.h5  model.25-1.85.h5   model.60-1.51.h5\n",
            "model.108-1.30.h5  model.25-1.94.h5   model.60-1.69.h5\n",
            "model.109-1.09.h5  model.25-1.98.h5   model.60-1.73.h5\n",
            "model.109-1.23.h5  model.25-2.01.h5   model.60-1.92.h5\n",
            "model.109-1.29.h5  model.25-2.10.h5   model.60-2.71.h5\n",
            "model.110-1.09.h5  model.25-2.55.h5   model.61-1.08.h5\n",
            "model.110-1.23.h5  model.25-2.61.h5   model.61-1.09.h5\n",
            "model.110-1.29.h5  model.25-2.67.h5   model.61-1.34.h5\n",
            "model.111-1.09.h5  model.25-2.73.h5   model.61-1.36.h5\n",
            "model.111-1.23.h5  model.25-2.75.h5   model.61-1.39.h5\n",
            "model.111-1.29.h5  model.26-1.28.h5   model.61-1.49.h5\n",
            "model.11-1.78.h5   model.26-1.35.h5   model.61-1.50.h5\n",
            "model.11-1.81.h5   model.26-1.40.h5   model.61-1.69.h5\n",
            "model.11-1.87.h5   model.26-1.44.h5   model.61-1.70.h5\n",
            "model.11-1.92.h5   model.26-1.52.h5   model.61-1.91.h5\n",
            "model.11-2.02.h5   model.26-1.53.h5   model.61-2.70.h5\n",
            "model.112-1.09.h5  model.26-1.54.h5   model.62-1.06.h5\n",
            "model.112-1.23.h5  model.26-1.55.h5   model.62-1.08.h5\n",
            "model.112-1.29.h5  model.26-1.56.h5   model.62-1.32.h5\n",
            "model.11-2.17.h5   model.26-1.57.h5   model.62-1.34.h5\n",
            "model.11-2.23.h5   model.26-1.59.h5   model.62-1.39.h5\n",
            "model.11-2.33.h5   model.26-1.65.h5   model.62-1.49.h5\n",
            "model.11-2.34.h5   model.26-1.66.h5   model.62-1.50.h5\n",
            "model.11-2.35.h5   model.26-1.74.h5   model.62-1.67.h5\n",
            "model.11-2.43.h5   model.26-1.82.h5   model.62-1.69.h5\n",
            "model.11-2.46.h5   model.26-1.91.h5   model.62-1.89.h5\n",
            "model.11-2.47.h5   model.26-1.96.h5   model.62-2.70.h5\n",
            "model.11-2.54.h5   model.26-1.98.h5   model.63-1.05.h5\n",
            "model.11-2.55.h5   model.26-2.08.h5   model.63-1.08.h5\n",
            "model.11-2.61.h5   model.26-2.51.h5   model.63-1.31.h5\n",
            "model.11-2.62.h5   model.26-2.53.h5   model.63-1.34.h5\n",
            "model.11-2.65.h5   model.26-2.61.h5   model.63-1.38.h5\n",
            "model.11-2.67.h5   model.26-2.67.h5   model.63-1.48.h5\n",
            "model.11-2.78.h5   model.26-2.71.h5   model.63-1.65.h5\n",
            "model.113-1.08.h5  model.26-2.75.h5   model.63-1.67.h5\n",
            "model.113-1.22.h5  model.27-1.25.h5   model.63-1.88.h5\n",
            "model.113-1.28.h5  model.27-1.37.h5   model.63-2.70.h5\n",
            "model.11-3.15.h5   model.27-1.45.h5   model.64-1.04.h5\n",
            "model.114-1.08.h5  model.27-1.48.h5   model.64-1.07.h5\n",
            "model.114-1.22.h5  model.27-1.52.h5   model.64-1.30.h5\n",
            "model.114-1.28.h5  model.27-1.53.h5   model.64-1.33.h5\n",
            "model.115-1.08.h5  model.27-1.54.h5   model.64-1.37.h5\n",
            "model.115-1.22.h5  model.27-1.59.h5   model.64-1.48.h5\n",
            "model.115-1.28.h5  model.27-1.64.h5   model.64-1.49.h5\n",
            "model.116-1.08.h5  model.27-1.66.h5   model.64-1.64.h5\n",
            "model.116-1.22.h5  model.27-1.69.h5   model.64-1.65.h5\n",
            "model.116-1.28.h5  model.27-1.80.h5   model.64-1.87.h5\n",
            "model.117-1.08.h5  model.27-1.88.h5   model.64-2.70.h5\n",
            "model.117-1.21.h5  model.27-1.93.h5   model.65-1.06.h5\n",
            "model.117-1.28.h5  model.27-1.95.h5   model.65-1.30.h5\n",
            "model.118-1.08.h5  model.27-2.06.h5   model.65-1.33.h5\n",
            "model.118-1.22.h5  model.27-2.47.h5   model.65-1.37.h5\n",
            "model.118-1.29.h5  model.27-2.49.h5   model.65-1.47.h5\n",
            "model.119-1.07.h5  model.27-2.61.h5   model.65-1.48.h5\n",
            "model.119-1.21.h5  model.27-2.67.h5   model.65-1.63.h5\n",
            "model.119-1.28.h5  model.27-2.70.h5   model.65-1.86.h5\n",
            "model.120-1.07.h5  model.27-2.75.h5   model.65-2.70.h5\n",
            "model.120-1.21.h5  model.28-1.25.h5   model.66-1.05.h5\n",
            "model.120-1.27.h5  model.28-1.37.h5   model.66-1.06.h5\n",
            "model.121-1.07.h5  model.28-1.44.h5   model.66-1.29.h5\n",
            "model.121-1.21.h5  model.28-1.47.h5   model.66-1.32.h5\n",
            "model.121-1.27.h5  model.28-1.48.h5   model.66-1.37.h5\n",
            "model.12-1.77.h5   model.28-1.49.h5   model.66-1.47.h5\n",
            "model.12-1.85.h5   model.28-1.50.h5   model.66-1.60.h5\n",
            "model.12-1.97.h5   model.28-1.51.h5   model.66-1.61.h5\n",
            "model.12-2.07.h5   model.28-1.58.h5   model.66-1.85.h5\n",
            "model.122-1.07.h5  model.28-1.64.h5   model.66-2.70.h5\n",
            "model.12-2.11.h5   model.28-1.66.h5   model.67-1.05.h5\n",
            "model.122-1.22.h5  model.28-1.78.h5   model.67-1.07.h5\n",
            "model.122-1.27.h5  model.28-1.85.h5   model.67-1.28.h5\n",
            "model.12-2.18.h5   model.28-1.91.h5   model.67-1.31.h5\n",
            "model.12-2.23.h5   model.28-1.93.h5   model.67-1.36.h5\n",
            "model.12-2.26.h5   model.28-2.03.h5   model.67-1.46.h5\n",
            "model.12-2.30.h5   model.28-2.43.h5   model.67-1.48.h5\n",
            "model.12-2.36.h5   model.28-2.47.h5   model.67-1.59.h5\n",
            "model.12-2.37.h5   model.28-2.61.h5   model.67-1.60.h5\n",
            "model.12-2.48.h5   model.28-2.67.h5   model.67-1.84.h5\n",
            "model.12-2.50.h5   model.28-2.68.h5   model.67-2.70.h5\n",
            "model.12-2.56.h5   model.28-2.75.h5   model.68-1.04.h5\n",
            "model.12-2.58.h5   model.29-1.25.h5   model.68-1.05.h5\n",
            "model.12-2.61.h5   model.29-1.40.h5   model.68-1.28.h5\n",
            "model.12-2.62.h5   model.29-1.44.h5   model.68-1.31.h5\n",
            "model.12-2.65.h5   model.29-1.46.h5   model.68-1.36.h5\n",
            "model.12-2.67.h5   model.29-1.48.h5   model.68-1.45.h5\n",
            "model.12-2.78.h5   model.29-1.51.h5   model.68-1.46.h5\n",
            "model.12-3.09.h5   model.29-1.59.h5   model.68-1.57.h5\n",
            "model.123-1.07.h5  model.29-1.60.h5   model.68-1.59.h5\n",
            "model.123-1.20.h5  model.29-1.63.h5   model.68-1.83.h5\n",
            "model.123-1.27.h5  model.29-1.76.h5   model.68-2.70.h5\n",
            "model.124-1.07.h5  model.29-1.82.h5   model.69-1.04.h5\n",
            "model.124-1.20.h5  model.29-1.89.h5   model.69-1.27.h5\n",
            "model.124-1.27.h5  model.29-1.90.h5   model.69-1.30.h5\n",
            "model.125-1.06.h5  model.29-2.00.h5   model.69-1.34.h5\n",
            "model.125-1.21.h5  model.29-2.39.h5   model.69-1.44.h5\n",
            "model.125-1.26.h5  model.29-2.44.h5   model.69-1.46.h5\n",
            "model.126-1.06.h5  model.29-2.61.h5   model.69-1.55.h5\n",
            "model.126-1.21.h5  model.29-2.66.h5   model.69-1.58.h5\n",
            "model.126-1.26.h5  model.29-2.67.h5   model.69-1.82.h5\n",
            "model.127-1.06.h5  model.29-2.75.h5   model.69-2.70.h5\n",
            "model.127-1.20.h5  model.30-1.25.h5   model.70-1.04.h5\n",
            "model.127-1.26.h5  model.30-1.41.h5   model.70-1.05.h5\n",
            "model.128-1.06.h5  model.30-1.42.h5   model.70-1.26.h5\n",
            "model.128-1.20.h5  model.30-1.43.h5   model.70-1.28.h5\n",
            "model.128-1.26.h5  model.30-1.45.h5   model.70-1.35.h5\n",
            "model.129-1.06.h5  model.30-1.46.h5   model.70-1.44.h5\n",
            "model.129-1.20.h5  model.30-1.54.h5   model.70-1.48.h5\n",
            "model.129-1.26.h5  model.30-1.55.h5   model.70-1.53.h5\n",
            "model.130-1.06.h5  model.30-1.56.h5   model.70-1.56.h5\n",
            "model.130-1.20.h5  model.30-1.57.h5   model.70-1.81.h5\n",
            "model.130-1.25.h5  model.30-1.60.h5   model.70-2.70.h5\n",
            "model.131-1.05.h5  model.30-1.64.h5   model.71-1.03.h5\n",
            "model.131-1.20.h5  model.30-1.75.h5   model.71-1.04.h5\n",
            "model.131-1.25.h5  model.30-1.79.h5   model.71-1.25.h5\n",
            "model.13-1.76.h5   model.30-1.87.h5   model.71-1.27.h5\n",
            "model.13-1.80.h5   model.30-2.01.h5   model.71-1.34.h5\n",
            "model.13-1.93.h5   model.30-2.36.h5   model.71-1.43.h5\n",
            "model.13-1.99.h5   model.30-2.42.h5   model.71-1.47.h5\n",
            "model.13-2.00.h5   model.30-2.61.h5   model.71-1.52.h5\n",
            "model.13-2.04.h5   model.30-2.63.h5   model.71-1.55.h5\n",
            "model.132-1.05.h5  model.30-2.67.h5   model.71-1.80.h5\n",
            "model.132-1.20.h5  model.30-2.74.h5   model.71-2.70.h5\n",
            "model.132-1.25.h5  model.31-1.26.h5   model.72-1.03.h5\n",
            "model.13-2.12.h5   model.31-1.39.h5   model.72-1.25.h5\n",
            "model.13-2.13.h5   model.31-1.40.h5   model.72-1.26.h5\n",
            "model.13-2.17.h5   model.31-1.41.h5   model.72-1.34.h5\n",
            "model.13-2.18.h5   model.31-1.43.h5   model.72-1.43.h5\n",
            "model.13-2.28.h5   model.31-1.45.h5   model.72-1.46.h5\n",
            "model.13-2.30.h5   model.31-1.51.h5   model.72-1.52.h5\n",
            "model.13-2.43.h5   model.31-1.55.h5   model.72-1.54.h5\n",
            "model.13-2.45.h5   model.31-1.56.h5   model.72-1.79.h5\n",
            "model.13-2.51.h5   model.31-1.61.h5   model.72-2.70.h5\n",
            "model.13-2.52.h5   model.31-1.63.h5   model.73-1.02.h5\n",
            "model.13-2.57.h5   model.31-1.67.h5   model.73-1.04.h5\n",
            "model.13-2.61.h5   model.31-1.72.h5   model.73-1.24.h5\n",
            "model.13-2.62.h5   model.31-1.77.h5   model.73-1.26.h5\n",
            "model.13-2.64.h5   model.31-1.84.h5   model.73-1.33.h5\n",
            "model.13-2.65.h5   model.31-1.85.h5   model.73-1.43.h5\n",
            "model.13-2.67.h5   model.31-1.97.h5   model.73-1.44.h5\n",
            "model.13-2.78.h5   model.31-2.32.h5   model.73-1.50.h5\n",
            "model.13-3.04.h5   model.31-2.40.h5   model.73-1.52.h5\n",
            "model.133-1.05.h5  model.31-2.60.h5   model.73-1.78.h5\n",
            "model.133-1.20.h5  model.31-2.61.h5   model.73-2.69.h5\n",
            "model.133-1.25.h5  model.31-2.67.h5   model.74-1.02.h5\n",
            "model.134-1.05.h5  model.31-2.74.h5   model.74-1.03.h5\n",
            "model.134-1.19.h5  model.32-1.33.h5   model.74-1.23.h5\n",
            "model.134-1.25.h5  model.32-1.35.h5   model.74-1.24.h5\n",
            "model.135-1.05.h5  model.32-1.39.h5   model.74-1.33.h5\n",
            "model.135-1.19.h5  model.32-1.41.h5   model.74-1.42.h5\n",
            "model.135-1.25.h5  model.32-1.43.h5   model.74-1.44.h5\n",
            "model.136-1.05.h5  model.32-1.47.h5   model.74-1.49.h5\n",
            "model.136-1.19.h5  model.32-1.49.h5   model.74-1.52.h5\n",
            "model.136-1.25.h5  model.32-1.55.h5   model.74-1.77.h5\n",
            "model.137-1.04.h5  model.32-1.62.h5   model.74-2.70.h5\n",
            "model.137-1.20.h5  model.32-1.63.h5   model.75-1.01.h5\n",
            "model.137-1.24.h5  model.32-1.64.h5   model.75-1.03.h5\n",
            "model.138-1.04.h5  model.32-1.71.h5   model.75-1.23.h5\n",
            "model.138-1.19.h5  model.32-1.74.h5   model.75-1.24.h5\n",
            "model.138-1.24.h5  model.32-1.81.h5   model.75-1.33.h5\n",
            "model.139-1.04.h5  model.32-1.83.h5   model.75-1.42.h5\n",
            "model.139-1.19.h5  model.32-1.96.h5   model.75-1.46.h5\n",
            "model.139-1.24.h5  model.32-2.30.h5   model.75-1.48.h5\n",
            "model.140-1.04.h5  model.32-2.38.h5   model.75-1.51.h5\n",
            "model.140-1.19.h5  model.32-2.57.h5   model.75-1.77.h5\n",
            "model.140-1.24.h5  model.32-2.61.h5   model.75-2.69.h5\n",
            "model.141-1.04.h5  model.32-2.67.h5   model.76-1.01.h5\n",
            "model.141-1.19.h5  model.32-2.74.h5   model.76-1.04.h5\n",
            "model.141-1.24.h5  model.33-1.31.h5   model.76-1.22.h5\n",
            "model.14-1.74.h5   model.33-1.34.h5   model.76-1.32.h5\n",
            "model.14-1.76.h5   model.33-1.36.h5   model.76-1.41.h5\n",
            "model.14-1.90.h5   model.33-1.39.h5   model.76-1.46.h5\n",
            "model.14-1.94.h5   model.33-1.41.h5   model.76-1.47.h5\n",
            "model.14-1.95.h5   model.33-1.45.h5   model.76-1.49.h5\n",
            "model.14-2.02.h5   model.33-1.52.h5   model.76-1.76.h5\n",
            "model.14-2.03.h5   model.33-1.60.h5   model.76-2.69.h5\n",
            "model.14-2.07.h5   model.33-1.62.h5   model.77-1.01.h5\n",
            "model.14-2.08.h5   model.33-1.64.h5   model.77-1.03.h5\n",
            "model.142-1.04.h5  model.33-1.65.h5   model.77-1.21.h5\n",
            "model.14-2.10.h5   model.33-1.70.h5   model.77-1.32.h5\n",
            "model.142-1.18.h5  model.33-1.72.h5   model.77-1.41.h5\n",
            "model.142-1.24.h5  model.33-1.77.h5   model.77-1.45.h5\n",
            "model.14-2.20.h5   model.33-1.79.h5   model.77-1.46.h5\n",
            "model.14-2.24.h5   model.33-1.81.h5   model.77-1.48.h5\n",
            "model.14-2.37.h5   model.33-1.96.h5   model.77-1.76.h5\n",
            "model.14-2.42.h5   model.33-2.27.h5   model.77-2.69.h5\n",
            "model.14-2.45.h5   model.33-2.35.h5   model.78-1.00.h5\n",
            "model.14-2.48.h5   model.33-2.54.h5   model.78-1.21.h5\n",
            "model.14-2.61.h5   model.33-2.61.h5   model.78-1.31.h5\n",
            "model.14-2.62.h5   model.33-2.67.h5   model.78-1.41.h5\n",
            "model.14-2.64.h5   model.33-2.74.h5   model.78-1.44.h5\n",
            "model.14-2.67.h5   model.34-1.31.h5   model.78-1.47.h5\n",
            "model.14-2.77.h5   model.34-1.33.h5   model.78-1.76.h5\n",
            "model.14-3.00.h5   model.34-1.34.h5   model.78-2.69.h5\n",
            "model.143-1.04.h5  model.34-1.39.h5   model.79-1.00.h5\n",
            "model.143-1.18.h5  model.34-1.40.h5   model.79-1.21.h5\n",
            "model.143-1.24.h5  model.34-1.44.h5   model.79-1.31.h5\n",
            "model.144-1.04.h5  model.34-1.45.h5   model.79-1.39.h5\n",
            "model.144-1.18.h5  model.34-1.51.h5   model.79-1.44.h5\n",
            "model.144-1.23.h5  model.34-1.60.h5   model.79-1.46.h5\n",
            "model.145-1.03.h5  model.34-1.61.h5   model.79-1.76.h5\n",
            "model.145-1.18.h5  model.34-1.68.h5   model.79-2.69.h5\n",
            "model.145-1.23.h5  model.34-1.69.h5   model.80-1.00.h5\n",
            "model.146-1.03.h5  model.34-1.70.h5   model.80-1.20.h5\n",
            "model.146-1.17.h5  model.34-1.76.h5   model.80-1.30.h5\n",
            "model.146-1.23.h5  model.34-1.77.h5   model.80-1.39.h5\n",
            "model.147-1.03.h5  model.34-1.79.h5   model.80-1.44.h5\n",
            "model.147-1.18.h5  model.34-1.93.h5   model.80-1.45.h5\n",
            "model.147-1.22.h5  model.34-2.23.h5   model.80-1.76.h5\n",
            "model.148-1.03.h5  model.34-2.32.h5   model.80-2.69.h5\n",
            "model.148-1.17.h5  model.34-2.51.h5   model.81-0.99.h5\n",
            "model.148-1.22.h5  model.34-2.61.h5   model.81-1.20.h5\n",
            "model.149-1.03.h5  model.34-2.67.h5   model.81-1.30.h5\n",
            "model.149-1.18.h5  model.34-2.74.h5   model.81-1.39.h5\n",
            "model.149-1.23.h5  model.35-1.29.h5   model.81-1.41.h5\n",
            "model.150-1.03.h5  model.35-1.31.h5   model.81-1.44.h5\n",
            "model.150-1.18.h5  model.35-1.37.h5   model.81-1.76.h5\n",
            "model.150-1.22.h5  model.35-1.38.h5   model.81-2.69.h5\n",
            "model.151-1.03.h5  model.35-1.41.h5   model.82-0.99.h5\n",
            "model.151-1.18.h5  model.35-1.44.h5   model.82-1.19.h5\n",
            "model.151-1.22.h5  model.35-1.53.h5   model.82-1.30.h5\n",
            "model.15-1.70.h5   model.35-1.60.h5   model.82-1.38.h5\n",
            "model.15-1.75.h5   model.35-1.66.h5   model.82-1.40.h5\n",
            "model.15-1.87.h5   model.35-1.67.h5   model.82-1.44.h5\n",
            "model.15-1.88.h5   model.35-1.68.h5   model.82-1.76.h5\n",
            "model.15-1.89.h5   model.35-1.70.h5   model.82-2.69.h5\n",
            "model.15-1.93.h5   model.35-1.75.h5   model.83-0.99.h5\n",
            "model.15-1.97.h5   model.35-1.77.h5   model.83-1.19.h5\n",
            "model.15-1.98.h5   model.35-1.78.h5   model.83-1.30.h5\n",
            "model.15-2.03.h5   model.35-1.92.h5   model.83-1.38.h5\n",
            "model.15-2.06.h5   model.35-2.19.h5   model.83-1.41.h5\n",
            "model.152-1.03.h5  model.35-2.30.h5   model.83-1.43.h5\n",
            "model.152-1.17.h5  model.35-2.49.h5   model.83-1.76.h5\n",
            "model.152-1.22.h5  model.35-2.61.h5   model.83-2.69.h5\n",
            "model.15-2.14.h5   model.35-2.67.h5   model.84-0.99.h5\n",
            "model.15-2.18.h5   model.35-2.74.h5   model.84-1.18.h5\n",
            "model.15-2.32.h5   model.36-1.28.h5   model.84-1.29.h5\n",
            "model.15-2.37.h5   model.36-1.29.h5   model.84-1.38.h5\n",
            "model.15-2.39.h5   model.36-1.35.h5   model.84-1.41.h5\n",
            "model.15-2.40.h5   model.36-1.36.h5   model.84-1.76.h5\n",
            "model.15-2.61.h5   model.36-1.38.h5   model.84-2.69.h5\n",
            "model.15-2.62.h5   model.36-1.41.h5   model.85-0.98.h5\n",
            "model.15-2.64.h5   model.36-1.48.h5   model.85-1.18.h5\n",
            "model.15-2.67.h5   model.36-1.50.h5   model.85-1.29.h5\n",
            "model.15-2.77.h5   model.36-1.54.h5   model.85-1.37.h5\n",
            "model.15-2.96.h5   model.36-1.60.h5   model.85-1.40.h5\n",
            "model.153-1.03.h5  model.36-1.65.h5   model.85-1.41.h5\n",
            "model.153-1.17.h5  model.36-1.66.h5   model.85-1.76.h5\n",
            "model.153-1.22.h5  model.36-1.70.h5   model.85-2.69.h5\n",
            "model.154-1.03.h5  model.36-1.72.h5   model.86-0.98.h5\n",
            "model.154-1.17.h5  model.36-1.77.h5   model.86-1.17.h5\n",
            "model.154-1.22.h5  model.36-1.80.h5   model.86-1.29.h5\n",
            "model.155-1.02.h5  model.36-1.82.h5   model.86-1.37.h5\n",
            "model.155-1.18.h5  model.36-2.16.h5   model.86-1.39.h5\n",
            "model.155-1.22.h5  model.36-2.26.h5   model.86-1.40.h5\n",
            "model.156-1.02.h5  model.36-2.47.h5   model.86-1.77.h5\n",
            "model.156-1.17.h5  model.36-2.61.h5   model.86-2.69.h5\n",
            "model.156-1.22.h5  model.36-2.67.h5   model.87-0.98.h5\n",
            "model.157-1.02.h5  model.36-2.73.h5   model.87-1.17.h5\n",
            "model.157-1.16.h5  model.37-1.27.h5   model.87-1.28.h5\n",
            "model.157-1.22.h5  model.37-1.29.h5   model.87-1.37.h5\n",
            "model.158-1.02.h5  model.37-1.32.h5   model.87-1.38.h5\n",
            "model.158-1.16.h5  model.37-1.34.h5   model.87-1.39.h5\n",
            "model.158-1.22.h5  model.37-1.38.h5   model.87-1.77.h5\n",
            "model.159-1.02.h5  model.37-1.45.h5   model.87-2.69.h5\n",
            "model.159-1.17.h5  model.37-1.52.h5   model.88-0.98.h5\n",
            "model.159-1.22.h5  model.37-1.55.h5   model.88-1.17.h5\n",
            "model.160-1.02.h5  model.37-1.61.h5   model.88-1.28.h5\n",
            "model.160-1.16.h5  model.37-1.63.h5   model.88-1.36.h5\n",
            "model.160-1.22.h5  model.37-1.64.h5   model.88-1.39.h5\n",
            "model.161-1.02.h5  model.37-1.70.h5   model.88-1.40.h5\n",
            "model.161-1.16.h5  model.37-1.71.h5   model.88-1.78.h5\n",
            "model.161-1.21.h5  model.37-1.75.h5   model.88-2.68.h5\n",
            "model.16-1.65.h5   model.37-1.82.h5   model.89-0.98.h5\n",
            "model.16-1.75.h5   model.37-2.12.h5   model.89-1.16.h5\n",
            "model.16-1.80.h5   model.37-2.22.h5   model.89-1.27.h5\n",
            "model.16-1.82.h5   model.37-2.45.h5   model.89-1.36.h5\n",
            "model.16-1.84.h5   model.37-2.61.h5   model.89-1.39.h5\n",
            "model.16-1.85.h5   model.37-2.73.h5   model.89-1.78.h5\n",
            "model.16-1.88.h5   model.38-1.23.h5   model.89-2.68.h5\n",
            "model.16-1.92.h5   model.38-1.25.h5   model.90-0.97.h5\n",
            "model.16-1.93.h5   model.38-1.33.h5   model.90-1.15.h5\n",
            "model.16-1.97.h5   model.38-1.36.h5   model.90-1.27.h5\n",
            "model.16-1.98.h5   model.38-1.48.h5   model.90-1.35.h5\n",
            "model.16-2.07.h5   model.38-1.49.h5   model.90-1.38.h5\n",
            "model.16-2.08.h5   model.38-1.52.h5   model.90-1.79.h5\n",
            "model.162-1.02.h5  model.38-1.57.h5   model.90-2.68.h5\n",
            "model.162-1.16.h5  model.38-1.59.h5   model.91-0.97.h5\n",
            "model.162-1.21.h5  model.38-1.61.h5   model.91-1.15.h5\n",
            "model.16-2.14.h5   model.38-1.62.h5   model.91-1.27.h5\n",
            "model.16-2.28.h5   model.38-1.68.h5   model.91-1.35.h5\n",
            "model.16-2.30.h5   model.38-1.74.h5   model.91-1.39.h5\n",
            "model.16-2.33.h5   model.38-1.80.h5   model.91-1.40.h5\n",
            "model.16-2.34.h5   model.38-1.87.h5   model.91-1.79.h5\n",
            "model.16-2.35.h5   model.38-2.09.h5   model.91-2.68.h5\n",
            "model.16-2.61.h5   model.38-2.19.h5   model.92-0.97.h5\n",
            "model.16-2.62.h5   model.38-2.42.h5   model.92-1.15.h5\n",
            "model.16-2.64.h5   model.38-2.61.h5   model.92-1.27.h5\n",
            "model.16-2.67.h5   model.38-2.73.h5   model.92-1.34.h5\n",
            "model.16-2.77.h5   model.39-1.24.h5   model.92-1.36.h5\n",
            "model.16-2.92.h5   model.39-1.25.h5   model.92-2.68.h5\n",
            "model.163-1.02.h5  model.39-1.31.h5   model.93-0.97.h5\n",
            "model.163-1.16.h5  model.39-1.36.h5   model.93-1.14.h5\n",
            "model.163-1.21.h5  model.39-1.50.h5   model.93-1.26.h5\n",
            "model.164-1.02.h5  model.39-1.51.h5   model.93-1.35.h5\n",
            "model.164-1.16.h5  model.39-1.57.h5   model.93-1.37.h5\n",
            "model.164-1.21.h5  model.39-1.60.h5   model.93-1.38.h5\n",
            "model.165-1.01.h5  model.39-1.66.h5   model.93-2.68.h5\n",
            "model.165-1.15.h5  model.39-1.72.h5   model.94-0.97.h5\n",
            "model.165-1.21.h5  model.39-1.76.h5   model.94-1.14.h5\n",
            "model.166-1.02.h5  model.39-2.06.h5   model.94-1.27.h5\n",
            "model.166-1.15.h5  model.39-2.15.h5   model.94-1.34.h5\n",
            "model.166-1.21.h5  model.39-2.40.h5   model.94-1.36.h5\n",
            "model.167-1.01.h5  model.39-2.61.h5   model.94-1.38.h5\n",
            "model.167-1.15.h5  model.39-2.73.h5   model.94-2.68.h5\n",
            "model.167-1.21.h5  model.40-1.21.h5   model.95-0.97.h5\n",
            "model.168-1.01.h5  model.40-1.22.h5   model.95-1.14.h5\n",
            "model.168-1.15.h5  model.40-1.30.h5   model.95-1.26.h5\n",
            "model.168-1.20.h5  model.40-1.32.h5   model.95-1.33.h5\n",
            "model.169-1.01.h5  model.40-1.33.h5   model.95-1.36.h5\n",
            "model.169-1.15.h5  model.40-1.43.h5   model.95-1.38.h5\n",
            "model.169-1.20.h5  model.40-1.54.h5   model.95-2.68.h5\n",
            "model.170-1.01.h5  model.40-1.57.h5   model.96-0.96.h5\n",
            "model.170-1.15.h5  model.40-1.58.h5   model.96-1.13.h5\n",
            "model.170-1.20.h5  model.40-1.59.h5   model.96-1.26.h5\n",
            "model.171-1.01.h5  model.40-1.64.h5   model.96-1.33.h5\n",
            "model.171-1.15.h5  model.40-1.72.h5   model.96-1.36.h5\n",
            "model.171-1.20.h5  model.40-2.04.h5   model.96-1.38.h5\n",
            "model.17-1.61.h5   model.40-2.11.h5   model.96-2.68.h5\n",
            "model.17-1.72.h5   model.40-2.37.h5   model.97-0.96.h5\n",
            "model.17-1.73.h5   model.40-2.61.h5   model.97-1.13.h5\n",
            "model.17-1.75.h5   model.40-2.73.h5   model.97-1.26.h5\n",
            "model.17-1.80.h5   model.41-1.20.h5   model.97-1.32.h5\n",
            "model.17-1.81.h5   model.41-1.21.h5   model.97-1.36.h5\n",
            "model.17-1.83.h5   model.41-1.28.h5   model.97-2.68.h5\n",
            "model.17-1.88.h5   model.41-1.30.h5   model.98-0.96.h5\n",
            "model.17-1.89.h5   model.41-1.33.h5   model.98-1.13.h5\n",
            "model.17-1.91.h5   model.41-1.38.h5   model.98-1.25.h5\n",
            "model.17-1.95.h5   model.41-1.57.h5   model.98-1.32.h5\n",
            "model.17-2.00.h5   model.41-1.58.h5   model.98-1.37.h5\n",
            "model.17-2.09.h5   model.41-1.63.h5   model.98-2.68.h5\n",
            "model.172-1.01.h5  model.41-1.69.h5   model.99-0.96.h5\n",
            "model.172-1.15.h5  model.41-2.01.h5   model.99-1.12.h5\n",
            "model.172-1.20.h5  model.41-2.08.h5   model.99-1.26.h5\n",
            "model.17-2.23.h5   model.41-2.35.h5   model.99-1.32.h5\n",
            "model.17-2.27.h5   model.41-2.61.h5   model.99-1.37.h5\n",
            "model.17-2.29.h5   model.41-2.73.h5   model.99-2.68.h5\n",
            "model.17-2.31.h5   model.42-1.21.h5   Ronash_DS_Assignment.csv\n",
            "model.17-2.61.h5   model.42-1.27.h5   \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "model.17-2.63.h5   model.42-1.28.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the csv file as a dataframe\n",
        "df = pd.read_csv('Ronash_DS_Assignment.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "722bMOmzF4_H",
        "outputId": "32625f3d-2f79-41f5-c3a1-15e0a2cd4247"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         product_id                                              title  \\\n",
              "0     3937721221199    Fidele Super Premium Adult Large Breed Dog Food   \n",
              "1     7353058033889                    Foldable Pet Toys Linen Storage   \n",
              "2     6594773549129                                     Bok Dok Diaper   \n",
              "3     4802008318014                              Tastybone Toy Chicken   \n",
              "4     1779705151539                Leather Leash Tab - Short Dog Leash   \n",
              "...             ...                                                ...   \n",
              "5265  4637089464407                              Candylab MOO Milk Van   \n",
              "5266  4996632444987  Truck - Modern Era Vehicles -- Red, White -  S...   \n",
              "5267  5528541003927  Car Sticker Flags Decal American Flag Sticker for   \n",
              "5268  1395163889730          Lazer Helmets Bayamo Pit Bull - Full Face   \n",
              "5269  3535679324240                             Deutz Agrotron Tractor   \n",
              "\n",
              "                 vendor                                               tags  \\\n",
              "0                Fidele  ['Adult', 'Bangalore', 'Chennai', 'Chicken', '...   \n",
              "1             Cap Point                                                 []   \n",
              "2             Pets Home  ['Brand_Pet Arabia', 'Category_Pets Home', 'Ca...   \n",
              "3             TastyBone                                                 []   \n",
              "4            Mighty Paw                 ['Leash', 'Leash Tab', 'Training']   \n",
              "...                 ...                                                ...   \n",
              "5265           Candylab  ['3 Years +', 'candylab', 'Discount Products',...   \n",
              "5266   Woodland Scenics  ['HO Scale', 'ho-scale-items', 'vehicles', 'wo...   \n",
              "5267        Cyan Selene                                          ['Other']   \n",
              "5268  OPEN BOX BARGAINS  ['65061090', 'Antiscratch Pinlock Ready Visor'...   \n",
              "5269               Siku  ['$0 to $25', 'diecast-models', 'gift-finder',...   \n",
              "\n",
              "                    category  \n",
              "0     Animals & Pet Supplies  \n",
              "1     Animals & Pet Supplies  \n",
              "2     Animals & Pet Supplies  \n",
              "3     Animals & Pet Supplies  \n",
              "4     Animals & Pet Supplies  \n",
              "...                      ...  \n",
              "5265        Vehicles & Parts  \n",
              "5266        Vehicles & Parts  \n",
              "5267        Vehicles & Parts  \n",
              "5268        Vehicles & Parts  \n",
              "5269        Vehicles & Parts  \n",
              "\n",
              "[5270 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1dc262ee-14ed-4fa4-a62e-2ec9314358bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_id</th>\n",
              "      <th>title</th>\n",
              "      <th>vendor</th>\n",
              "      <th>tags</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3937721221199</td>\n",
              "      <td>Fidele Super Premium Adult Large Breed Dog Food</td>\n",
              "      <td>Fidele</td>\n",
              "      <td>['Adult', 'Bangalore', 'Chennai', 'Chicken', '...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7353058033889</td>\n",
              "      <td>Foldable Pet Toys Linen Storage</td>\n",
              "      <td>Cap Point</td>\n",
              "      <td>[]</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6594773549129</td>\n",
              "      <td>Bok Dok Diaper</td>\n",
              "      <td>Pets Home</td>\n",
              "      <td>['Brand_Pet Arabia', 'Category_Pets Home', 'Ca...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4802008318014</td>\n",
              "      <td>Tastybone Toy Chicken</td>\n",
              "      <td>TastyBone</td>\n",
              "      <td>[]</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1779705151539</td>\n",
              "      <td>Leather Leash Tab - Short Dog Leash</td>\n",
              "      <td>Mighty Paw</td>\n",
              "      <td>['Leash', 'Leash Tab', 'Training']</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>4637089464407</td>\n",
              "      <td>Candylab MOO Milk Van</td>\n",
              "      <td>Candylab</td>\n",
              "      <td>['3 Years +', 'candylab', 'Discount Products',...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5266</th>\n",
              "      <td>4996632444987</td>\n",
              "      <td>Truck - Modern Era Vehicles -- Red, White -  S...</td>\n",
              "      <td>Woodland Scenics</td>\n",
              "      <td>['HO Scale', 'ho-scale-items', 'vehicles', 'wo...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5267</th>\n",
              "      <td>5528541003927</td>\n",
              "      <td>Car Sticker Flags Decal American Flag Sticker for</td>\n",
              "      <td>Cyan Selene</td>\n",
              "      <td>['Other']</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5268</th>\n",
              "      <td>1395163889730</td>\n",
              "      <td>Lazer Helmets Bayamo Pit Bull - Full Face</td>\n",
              "      <td>OPEN BOX BARGAINS</td>\n",
              "      <td>['65061090', 'Antiscratch Pinlock Ready Visor'...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5269</th>\n",
              "      <td>3535679324240</td>\n",
              "      <td>Deutz Agrotron Tractor</td>\n",
              "      <td>Siku</td>\n",
              "      <td>['$0 to $25', 'diecast-models', 'gift-finder',...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5270 rows  5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dc262ee-14ed-4fa4-a62e-2ec9314358bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1dc262ee-14ed-4fa4-a62e-2ec9314358bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1dc262ee-14ed-4fa4-a62e-2ec9314358bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "CbII9Lq6CYV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of each label\n",
        "df['category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E1ZDXyPlr6y",
        "outputId": "e8e77ee1-b59e-49fd-cdf8-0f09550c6673"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apparel & Accessories        1000\n",
              "Animals & Pet Supplies        500\n",
              "Food, Beverages & Tobacco     400\n",
              "Sporting Goods                400\n",
              "Luggage & Bags                400\n",
              "Home & Garden                 400\n",
              "Health & Beauty               400\n",
              "Media                         300\n",
              "Toys & Games                  300\n",
              "Furniture                     200\n",
              "Baby & Toddler                200\n",
              "Arts & Entertainment          200\n",
              "Electronics                   100\n",
              "Business & Industrial         100\n",
              "Office Supplies               100\n",
              "Vehicles & Parts              100\n",
              "Hardware                       50\n",
              "Cameras & Optics               50\n",
              "Software                       50\n",
              "Religious & Ceremonial         20\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting how many indices are duplicated in each column\n",
        "print(f\"There are {sum(df['title'].duplicated())} duplicate title.\")\n",
        "print(f\"There are {sum(df['vendor'].duplicated())} duplicate vondor.\")\n",
        "print(f\"There are {sum(df['tags'].duplicated())} duplicate tags.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_ggKoLUAEf",
        "outputId": "22e93ce4-352b-4d1c-9a1a-f5a193551569"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 duplicate title.\n",
            "There are 1256 duplicate vondor.\n",
            "There are 716 duplicate tags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of Nan samples\n",
        "df.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD95efZwCRYT",
        "outputId": "8c49657f-f644-4aeb-98c6-609f9dcc53af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are just 3 Nan samples in the dataset and we delete them."
      ],
      "metadata": {
        "id": "rhPJQuDEb8oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Dataset"
      ],
      "metadata": {
        "id": "PT6z8R1sCeXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edited\n",
        "# Remove 3 rows with null values on the current dataframe.\n",
        "df.dropna(axis=0, inplace=True)\n",
        "# check if null values are removed properly.\n",
        "df.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHsosYjU2BeI",
        "outputId": "6f6aa349-beaf-4e3b-8b8a-d817c9914d5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the function for extracting and standardizing the sentences\n",
        "def text_extraction(dfi):\n",
        "  # in this function, we concatenate text feature parts of the data as a sentence\n",
        "  sentence = ' '.join([dfi['title'], str(dfi['vendor']), dfi['tags']])\n",
        "  # Remove punctuations\n",
        "  sentence = re.sub('[^a-zA-Z0-9$.]', ' ', sentence)\n",
        "  # Single character removal\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "  # Changint to lowercase\n",
        "  sentence = sentence.lower()\n",
        "  return sentence\n",
        "\n",
        "# printing 10 sample sentences\n",
        "for i in range(10):\n",
        "  print(text_extraction(df.iloc[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH9UzP7-__P2",
        "outputId": "499e1444-a99c-41a7-9ae0-807e2fbdda5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fidele super premium adult large breed dog food fidele adult bangalore chennai chicken doberman dog dry foods fidele german shepherd golden retriever great dane highpriority imported labrador less than 1000 less than 2000 less than 500 mastiff orange pet nutrition \n",
            "foldable pet toys linen storage cap point \n",
            "bok dok diaper pets home brand pet arabia category pets home category small pets supplies type pet home type pet supplies \n",
            "tastybone toy chicken tastybone \n",
            "leather leash tab short dog leash mighty paw leash leash tab training \n",
            "pridebites texas guitar dog toy pride bites brand pridebites toy type plush \n",
            "burns sensitive pork potato burns 10 25 25 50 50 75 adult burns coat dog food food delivery jansale18 natural nonsale19 sensitive size 12kg size 2kg size 6kg skin \n",
            "bully sticks dog toy adog.co bully sticks dog chew toys dog toys \n",
            "kazoo tough giraffe dog toy kazoo brand kazoo june2021 kazoo material plush plush \n",
            "orgo dog biscuits fresh milk petku brand orgo category dogs dogs lifestage all lifestages orgo price rp 0 to rp 100.000 subcategory treats treats \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dataset\n",
        "dataset = pd.DataFrame(columns=['text', 'label'])\n",
        "for i in range(len(df)):\n",
        "  dataset = dataset.append({'text':text_extraction(df.iloc[i]), 'label':df.iloc[i]['category']}, ignore_index = True)\n",
        "\n",
        "# creating integer labels for multiclass training\n",
        "dataset['label_int'] = pd.Categorical(dataset['label']).codes\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7x4WG7NC9Xz3",
        "outputId": "7ae12003-e182-47e9-c4ef-1cdcf70f7372"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0     fidele super premium adult large breed dog foo...   \n",
              "1            foldable pet toys linen storage cap point    \n",
              "2     bok dok diaper pets home brand pet arabia cate...   \n",
              "3                      tastybone toy chicken tastybone    \n",
              "4     leather leash tab short dog leash mighty paw l...   \n",
              "...                                                 ...   \n",
              "5262  candylab moo milk van candylab 3 years candyla...   \n",
              "5263  truck modern era vehicles red white scale ho w...   \n",
              "5264  car sticker flags decal american flag sticker ...   \n",
              "5265  lazer helmets bayamo pit bull full face open b...   \n",
              "5266  deutz agrotron tractor siku $0 to $25 diecast ...   \n",
              "\n",
              "                       label  label_int  \n",
              "0     Animals & Pet Supplies          0  \n",
              "1     Animals & Pet Supplies          0  \n",
              "2     Animals & Pet Supplies          0  \n",
              "3     Animals & Pet Supplies          0  \n",
              "4     Animals & Pet Supplies          0  \n",
              "...                      ...        ...  \n",
              "5262        Vehicles & Parts         19  \n",
              "5263        Vehicles & Parts         19  \n",
              "5264        Vehicles & Parts         19  \n",
              "5265        Vehicles & Parts         19  \n",
              "5266        Vehicles & Parts         19  \n",
              "\n",
              "[5267 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-982372ca-fd79-4b0b-900f-eb544fc92b4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fidele super premium adult large breed dog foo...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>foldable pet toys linen storage cap point</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bok dok diaper pets home brand pet arabia cate...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tastybone toy chicken tastybone</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>leather leash tab short dog leash mighty paw l...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5262</th>\n",
              "      <td>candylab moo milk van candylab 3 years candyla...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5263</th>\n",
              "      <td>truck modern era vehicles red white scale ho w...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5264</th>\n",
              "      <td>car sticker flags decal american flag sticker ...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>lazer helmets bayamo pit bull full face open b...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5266</th>\n",
              "      <td>deutz agrotron tractor siku $0 to $25 diecast ...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5267 rows  3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-982372ca-fd79-4b0b-900f-eb544fc92b4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-982372ca-fd79-4b0b-900f-eb544fc92b4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-982372ca-fd79-4b0b-900f-eb544fc92b4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the names of the labels\n",
        "labels_names = list(Counter(dataset['label']).keys())\n",
        "labels_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uHdTY3p9Xfr",
        "outputId": "5d05de5d-4674-4a61-fef6-90185927214e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Animals & Pet Supplies',\n",
              " 'Apparel & Accessories',\n",
              " 'Arts & Entertainment',\n",
              " 'Baby & Toddler',\n",
              " 'Business & Industrial',\n",
              " 'Cameras & Optics',\n",
              " 'Electronics',\n",
              " 'Food, Beverages & Tobacco',\n",
              " 'Furniture',\n",
              " 'Hardware',\n",
              " 'Health & Beauty',\n",
              " 'Home & Garden',\n",
              " 'Luggage & Bags',\n",
              " 'Media',\n",
              " 'Office Supplies',\n",
              " 'Religious & Ceremonial',\n",
              " 'Software',\n",
              " 'Sporting Goods',\n",
              " 'Toys & Games',\n",
              " 'Vehicles & Parts']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing each integer label and its corresponding name label\n",
        "for i, label in enumerate(labels_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXCgd6QTC_k6",
        "outputId": "516acc65-674d-43b4-a95d-b5a565fe6ee7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to Animals & Pet Supplies\n",
            "Label 1 corresponds to Apparel & Accessories\n",
            "Label 2 corresponds to Arts & Entertainment\n",
            "Label 3 corresponds to Baby & Toddler\n",
            "Label 4 corresponds to Business & Industrial\n",
            "Label 5 corresponds to Cameras & Optics\n",
            "Label 6 corresponds to Electronics\n",
            "Label 7 corresponds to Food, Beverages & Tobacco\n",
            "Label 8 corresponds to Furniture\n",
            "Label 9 corresponds to Hardware\n",
            "Label 10 corresponds to Health & Beauty\n",
            "Label 11 corresponds to Home & Garden\n",
            "Label 12 corresponds to Luggage & Bags\n",
            "Label 13 corresponds to Media\n",
            "Label 14 corresponds to Office Supplies\n",
            "Label 15 corresponds to Religious & Ceremonial\n",
            "Label 16 corresponds to Software\n",
            "Label 17 corresponds to Sporting Goods\n",
            "Label 18 corresponds to Toys & Games\n",
            "Label 19 corresponds to Vehicles & Parts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrames"
      ],
      "metadata": {
        "id": "EXOx96NLDD98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataset to train, validation, and test dataframes\n",
        "train_df, test_df= train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of samples in training set: {len(train_df)}\")\n",
        "print(f\"Number of samples in validation set: {len(val_df)}\")\n",
        "print(f\"Number of samples in test set: {len(test_df)}\")\n",
        "\n",
        "# extracting texts and labels from dataframes\n",
        "train_texts = train_df['text']\n",
        "train_labels = train_df['label_int']\n",
        "val_texts = val_df['text']\n",
        "val_labels = val_df['label_int']\n",
        "test_texts = test_df['text']\n",
        "test_labels = test_df['label_int']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWd2Rfp69X8g",
        "outputId": "7d47cbc0-a383-4727-cb84-5d9dcc9264cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in training set: 4213\n",
            "Number of samples in validation set: 527\n",
            "Number of samples in test set: 527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data generators with batch size 32\n",
        "batch_size = 50\n",
        "raw_train_batch = tf.data.Dataset.from_tensor_slices((train_texts, train_labels)).batch(batch_size)\n",
        "raw_val_batch = tf.data.Dataset.from_tensor_slices((val_texts, val_labels)).batch(batch_size)\n",
        "raw_test_batch = tf.data.Dataset.from_tensor_slices((test_texts, test_labels)).batch(batch_size)\n",
        "\n",
        "# printing texts and labels of a batch of raw train\n",
        "for text, label in raw_train_batch.take(1):\n",
        "  print('Texts: {}'.format(text))\n",
        "  print('labels: {}'.format(label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kA4X60zkC3",
        "outputId": "0548a7ca-e76f-48ca-a7cf-6ddc68c17020"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts: [b'seachem multitest ammonia seachem '\n",
            " b'smart electric cat teasing toy pet clever cat toys cats rr track catproducts rr track cattoys '\n",
            " b'ocean hunter wood sculpture novica united inc assembly required 492 assembly requiredstr no assembly required class home accessories classid 581 color 5829 colorstr brown department decor departmentid 542 margin 60 material style 33 stylestr modern subclass home accessories subclassid 793 '\n",
            " b'impala rollerskates leopard impala cheetah impala leopard red roller skate rollerskate skate skates vegan '\n",
            " b'diy personalised dreamcatcher activity kit sea animal doxbox store '\n",
            " b'treasure by estetica remy human hair wig closeout estetica liquid show percent off show percent off '\n",
            " b'etching cl grenadine drapery upholstery fabric by kaufmann kaufmann hidden pk ikat khaki multi color print red shop fabric throw pillows trim wallpaper ikat shop fabric throw pillows trim wallpaper kaufmann shop fabric throw pillows trim wallpaper prints '\n",
            " b'bleed proof white opaque watercolor paint dr. ph martin brand drphmartin dr. ph martin flashsale0621 medium medium artjournaling paint priority tutorial fallpumpkin tutorial fancyfeather tutorial galaxywolf tutorial nebula type paint '\n",
            " b'party suede peep toe embellished chunky heel sandals meet yours fashion chunky heel sandals club sandals dress sandals fashion sandals heel sandals high heel sandals new arrivals open toe sandals party sandals peep toe sandals plus size sandals sandals suede sandals summer sandals '\n",
            " b'sea grass rolling tip empire glassworks rolling tip unfinished '\n",
            " b'interactive iq treat ball toy for dogs cats cj feeder waterer pet supplies '\n",
            " b'danek black nubuck stevemadden danek black nubuck danek black boots brand blondo category men shoes cf size 10 cf size 10 5 cf size 11 cf size 12 cf size 7 5 cf size 8 cf size 8 5 cf size 9 cf size 9 5 color black flat 0 1 inch full price in stock ready to ship low 1 2 inches men new price over $150 scm category boots men style lace up style leather style waterproof type boots waterproof '\n",
            " b'just female hilda pleated skirt oranges print just female hilda just female midi pleated sheer spo disabled '\n",
            " b'natural factors sambucus black elderberry natural factors 068958047072 cold flu treatment immune support new nf04707 '\n",
            " b'the triumph iii briefcase lotuff leather '\n",
            " b'p.l.a.y. zoomierex fantastug sea foam dog toy p.l.a.y. brand p.l.a.y. summer collection toy type fetch toy type floating toy type rubber toy type tug twr '\n",
            " b'luggage tag nantucket maya traditions accessories all accessories collection all wholesale collection bag charms bis hidden by recipient for galentines by recipient for the long distance friend filter artisan maya traditions filter color green filter country guatemala filter materials cotton finalsale cart flexify gift guide under $25 collection graduation gifts collection green guatemala last chance collection luggage tags maya traditions maya traditions collections travel travel accessories collection wholesale wholesale travel collection wipd2020 woven '\n",
            " b'wide openning diaper bag backpack maroon chronos baby bag baby bag for mommy baby bags baby bags for mom baby changing bag baby diaper bag backpack bag bags changing bag designer bag diaper bag diaper bag backpack fashion bag free shipping bags maternity bag mom bag mommy bag mummy bag nappy bag stroller bag travel bag travel bag organizer waterproof bag '\n",
            " b'knorr potato mushroom steinpilz soup knorr brands knorr categories soups countries germany cream german germany ingredients cream ingredients mushroom ingredients potatoes knorr pantry seasonings spices spice storage pantry '\n",
            " b'inkolo side table uniqwa category furniture category occasional tables category tables deposit latest item range african furniture range island resort range organic '\n",
            " b'jabberwocky jellybean games availability in stock category children google true inv product type board games '\n",
            " b'black agate ring large in charge power stone adjustable bronze dig crystals agate black bronze chakra root chakra color black color bronze intentions protection jewelry jewelry rings protection crystals ring size adjustable rings rings metal root chakra stone agate type jewelry '\n",
            " b'tabs tattoo artist barrier sheets hextat barrier covid hextat medical '\n",
            " b'dog pakiet dnp pc skin support pick me pets brand dolina noteci category pet supplies type dog food '\n",
            " b'organic lace ruffle bib various colors ovedbaby bib bibs sale '\n",
            " b'silver white clear rhinestones metal band one size all decd out one size band rhinestones silver '\n",
            " b'hanover lavallette umbrella in blue lavalletteumb hanover hanover outdoor furniture umbrellas '\n",
            " b'braided bone orange tall tails brand tall tails pet dogs price $20.00 $29.99 type interactive toys '\n",
            " b'brother bond cask strength bourbon whiskey brother bond tab1 about brothers bond brothers bond engraving fall flavors father day memorial day size 750ml whiskey youngs '\n",
            " b'imex phillips drive drywall zinc screws imex imex screws washers tools '\n",
            " b'playground patchwork backpack primecut '\n",
            " b'giant jenga block game all things for kids giant games in stock outdoor games spo disabled wooden toys '\n",
            " b'hal leonard perfect pitch method hal leonard always show discount allowed for instrument any hal leonard lessons practice music theory reference sheet music any sheet music category lessons practice sheet music category theory reference spo default spo enabled '\n",
            " b'ken ultra premium egg yolk flake ken fish food flake food kens kens ultra '\n",
            " b'carlton unisex squash badminton running goggle carlton carlton r cricket '\n",
            " b'drill knowledge battling vs the wall infamous paintball advanced video kevin kali rudulph members everything members only members only video '\n",
            " b'shimmer silk crinkle chiffon black the fabric store online category crinkle chiffon category sheer category silk category woven color black color metallic colour black colour metallic print no pattern product fabric product wovens project dresses tunics project occasionwear project shirts tops remnant sale stretch non stretch weight lightweight width 126 155cm width 50 60 '\n",
            " b'warrior official canada lacrosse game ball warrior ball canada game lacrosse official warrior white '\n",
            " b'duca by matiste fermo duca by matiste '\n",
            " b'flat hoop matsu jewellery handmade hoops silver silver earring silver stud sterling silver sterling silver stud '\n",
            " b'car culture hat chemical guys best chemical guys clothing gift hat hats merchandise new '\n",
            " b'lettia memory foam clik all purpose girth lettia dropship union girths horse new tack '\n",
            " b'iron flamingo pink lady belgian wit ale iron flamingo beer cola iron flamingo '\n",
            " b'finner bench arteriors benches new '\n",
            " b'sleeping bag storage sack mont dry bags and accessories packs sleeping bag sleeping bag storage sack sleeping bags '\n",
            " b'retractable everywhere table tennis set ecomstock baby toddler toys '\n",
            " b'euca poplin super random dot toko ncit 100 tencel 714a 715d composition tencel fabric woven type print '\n",
            " b'nfl rico industries bling chrome license plate frame with glitter accent new... rico industries 12.25 inches accent afco afco au afco aub00j10lv9o bling chrome color fcgl1401 featured from research featured item frame giants glitter industries license new nfl plate price 20 rico ship to us team with york '\n",
            " b'sunrise bow tie puccissime pet couture handmade dog accessories luxury dog products pet supplies '\n",
            " b'inflatable backpacking sleeping pad with pillow camping hiking mat harry jean backpacking sleeping pad best air mattress for camping best backpacking sleeping pad best camping bed best camping mattress best camping sleeping pad best sleeping pad camping air bed camping mat camping mattress camping mattress pad camping pad camping sleeping mats camping sleeping pad inflatable camping mattress sleeping mat sleeping pad sleeping pads for camping ']\n",
            "labels: [ 0  0 11 17  0  1  2  9  1 11  0  1  1 10 12  0 12 12  7  8 18  1  4  0\n",
            "  3  1  8  0  7  9 12 18 13  0 17  6  2 17  1  1  1 17  7  8 17  3  2 17\n",
            "  0 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information"
      ],
      "metadata": {
        "id": "NwpAejboDvHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# counting how many words are there in the whole texts of the dataset\n",
        "num_of_words = 0\n",
        "for i in dataset['text']: num_of_words += len(i.split())\n",
        "\n",
        "print('Number of words: {}'.format(num_of_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2QpUriFrsU_",
        "outputId": "f630d3b7-6dbb-4a1f-8f1e-85a059597dcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 112524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are about 112000 words in the texts.\n",
        "\n"
      ],
      "metadata": {
        "id": "wfw3CqfZsWyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edited\n",
        "# Counting max sequence length and how many non-repetitive words are there in the whole texts of the dataset\n",
        "# Remove words repeated more that threshold. --> Remove outlier words\n",
        "\n",
        "# Variables\n",
        "non_repetitive_list = [] # List of all non repetitive words\n",
        "words = {}               # Store all the words\n",
        "max_seq_lenght = 0       # Max sequence\n",
        "threshold = 800\n",
        "\n",
        "# Iterate rows\n",
        "for row in dataset.itertuples(index=True):\n",
        "  index = getattr(row,'Index')\n",
        "  text = getattr(row,'text')\n",
        "  new_text = text \n",
        "\n",
        "  for each_word in text.split():\n",
        "    if each_word not in non_repetitive_list: non_repetitive_list.append(each_word)\n",
        "\n",
        "    # Update the dictionary and remove the word passing the threshold \n",
        "    # from keras.preprocessing.text import Tokenizer, could also be used \n",
        "    if each_word in words.keys():\n",
        "      if  words[each_word] >= threshold: \n",
        "        new_text = new_text.replace(each_word, '')\n",
        "      value = words[each_word]\n",
        "      words.update({each_word: value+1})\n",
        "    else: words[each_word]=1\n",
        "  \n",
        "  dataset.at[index, 'text'] = new_text \n",
        "  lenght = len(new_text.split())\n",
        "  if lenght > max_seq_lenght: max_seq_lenght = lenght\n",
        "\n",
        "print('Max sequence length: {}'.format(max_seq_lenght))\n",
        "print('Non-repetitive words: {}'.format(len(non_repetitive_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ufQzaF9ncqZ",
        "outputId": "2db35cd8-e29d-4ebc-b785-c6526dc7262f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 309\n",
            "Non-repetitive words: 18927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum sequence length is 309 and There are about 19000 non-repetitive words in the whole dataset texts. So we set max word features to 10000 and sequence length to 310."
      ],
      "metadata": {
        "id": "cGGf8rQorkSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "uzLtURj5EeEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the text vectorization layer with 10000 words and 310 sequence length\n",
        "max_features = 10000\n",
        "sequence_length = max_seq_lenght + 1\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# fitting the state of the preprocessing layer to the train set. This will cause the model to build an index of strings to integers.\n",
        "vectorize_layer.adapt(train_texts)\n",
        "\n",
        "# defining the vectorize text function\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# retrieving a sample from a batch of texts and labels from the train set\n",
        "text_batch, label_batch = next(iter(raw_train_batch))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", first_label)\n",
        "print(\"Vectorized text\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey2QJjO97T9P",
        "outputId": "32a63409-6443-4b7c-b06a-3db5c4bebc3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b'seachem multitest ammonia seachem ', shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int8)\n",
            "Vectorized text (<tf.Tensor: shape=(1, 310), dtype=int64, numpy=\n",
            "array([[2075, 6580,    1, 2075,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0]])>, <tf.Tensor: shape=(), dtype=int8, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting corresponding word of each integer \n",
        "print(\"1401 ---> \",vectorize_layer.get_vocabulary()[1401])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X84dkB768GSx",
        "outputId": "a4581774-9d2a-4fc8-c924-5aee60c2486a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1401 --->  moderate\n",
            " 313 --->  soft\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model's Input "
      ],
      "metadata": {
        "id": "kj9xHgbGE7Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating train, val, and test vectorized dataset and prefetching them\n",
        "train_ds = raw_train_batch.map(vectorize_text)\n",
        "val_ds = raw_val_batch.map(vectorize_text)\n",
        "test_ds = raw_test_batch.map(vectorize_text)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "DiW717GQ_77D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edited\n",
        "train_text = []\n",
        "train_labels = []\n",
        "for text, labels in train_ds:\n",
        "  train_text.append(text)\n",
        "  train_labels.append(labels)\n",
        "\n",
        "test_text = []\n",
        "test_labels = []\n",
        "for text, labels in test_ds:\n",
        "  test_text.append(text)\n",
        "  test_labels.append(labels)"
      ],
      "metadata": {
        "id": "fV18RCKjRFxf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A parameter grid for XGBoost\n",
        "# params = {\n",
        "#         'min_child_weight': [1, 5, 10],\n",
        "#         'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "#         'subsample': [0.6, 0.8, 1.0],\n",
        "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "#         'max_depth': [3, 4, 5]\n",
        "#         }\n",
        "# xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
        "#                     silent=True, nthread=1)\n",
        "# folds = 3\n",
        "# param_comb = 5\n",
        "# # train_texts_df, train_labels_df\n",
        "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
        "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, verbose=3, random_state=1001 )\n",
        "# random_search.fit(torch.Tensor(train_text), torch.Tensor(train_labels))"
      ],
      "metadata": {
        "id": "kGrxY49bBcfa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edited\n",
        "def create_model(num_filters, kernel_size, vocab_size, sequence_length, embedding_dim,hidden_dims,num_of_classes):\n",
        "  # Created Embedding (Input) Layer (max_words) --> Convolutional Layer\n",
        "  model = tf.keras.Sequential([\n",
        "  layers.Embedding(input_dim=vocab_size,                              # input_dim: the size of the vocabulary\n",
        "                   output_dim=embedding_dim,                          # output_dim: the size of the dense vector\n",
        "                   input_length=sequence_length),                     # input_length: the length of the sequence\n",
        "  \n",
        "\n",
        "  # Create the convolutional layer\n",
        "  layers.Conv1D(num_filters, kernel_size, padding='valid', activation='relu', strides=1),\n",
        "  # Create the pooling layer\n",
        "  layers.GlobalMaxPool1D(),\n",
        "  layers.Dropout(0.95),\n",
        "  # layers.Dense(hidden_dims),\n",
        "  # layers.Activation('relu'),\n",
        "  # layers.Dropout(0.75),\n",
        "\n",
        "  # Create the output layer\n",
        "  layers.Dense(num_of_classes),\n",
        "  layers.Activation('softmax'),\n",
        "  ])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "5eczALl5KZNI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edited\n",
        "# model configuration\n",
        "\n",
        "# hyper parameters\n",
        "epochs = 200\n",
        "vocab_size = len(vectorize_layer.get_vocabulary())\n",
        "\n",
        "\n",
        "# num_filters, kernel_size, vocab_size, sequence_length, embedding_dim,hidden_dims,num_of_classes\n",
        "model = create_model(250, 5, vocab_size, sequence_length, 50, 150, len(labels_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBHmGG_SAWhp",
        "outputId": "12524458-75ca-4ee9-9920-29b1ab0112cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 310, 50)           500000    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 306, 250)          62750     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 250)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 250)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                5020      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 20)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 567,770\n",
            "Trainable params: 567,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialzing Model"
      ],
      "metadata": {
        "id": "m7nPZlvUFJSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.007, momentum=0.8, nesterov=True)\n",
        "\n",
        "# model compilation\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uL6c-YGfAwLz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "eEE6yxpOFUoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "]\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    verbose= True,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIfmPA3SvX4d",
        "outputId": "a3d8f450-1ee9-4878-9161-eb5f82160182"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "85/85 [==============================] - 3s 28ms/step - loss: 2.9281 - accuracy: 0.1251 - val_loss: 2.8176 - val_accuracy: 0.1992\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 2.8286 - accuracy: 0.1818 - val_loss: 2.7464 - val_accuracy: 0.1992\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7854 - accuracy: 0.1854 - val_loss: 2.7103 - val_accuracy: 0.1992\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7541 - accuracy: 0.1818 - val_loss: 2.6854 - val_accuracy: 0.1992\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7517 - accuracy: 0.1856 - val_loss: 2.6748 - val_accuracy: 0.1992\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7414 - accuracy: 0.1868 - val_loss: 2.6669 - val_accuracy: 0.1992\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7277 - accuracy: 0.1868 - val_loss: 2.6588 - val_accuracy: 0.1992\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7250 - accuracy: 0.1878 - val_loss: 2.6546 - val_accuracy: 0.1992\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7149 - accuracy: 0.1875 - val_loss: 2.6493 - val_accuracy: 0.1992\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7031 - accuracy: 0.1878 - val_loss: 2.6444 - val_accuracy: 0.1992\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.7034 - accuracy: 0.1875 - val_loss: 2.6412 - val_accuracy: 0.1992\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6998 - accuracy: 0.1880 - val_loss: 2.6361 - val_accuracy: 0.1992\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6968 - accuracy: 0.1880 - val_loss: 2.6341 - val_accuracy: 0.1992\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6874 - accuracy: 0.1880 - val_loss: 2.6277 - val_accuracy: 0.1992\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6829 - accuracy: 0.1873 - val_loss: 2.6232 - val_accuracy: 0.1992\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6783 - accuracy: 0.1875 - val_loss: 2.6177 - val_accuracy: 0.1992\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6744 - accuracy: 0.1882 - val_loss: 2.6126 - val_accuracy: 0.1992\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6709 - accuracy: 0.1882 - val_loss: 2.6078 - val_accuracy: 0.1992\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6646 - accuracy: 0.1894 - val_loss: 2.6003 - val_accuracy: 0.1992\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6600 - accuracy: 0.1923 - val_loss: 2.5931 - val_accuracy: 0.1992\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6497 - accuracy: 0.1932 - val_loss: 2.5821 - val_accuracy: 0.1992\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6426 - accuracy: 0.1949 - val_loss: 2.5706 - val_accuracy: 0.1992\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6292 - accuracy: 0.2003 - val_loss: 2.5543 - val_accuracy: 0.1992\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.6175 - accuracy: 0.2067 - val_loss: 2.5322 - val_accuracy: 0.2201\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.5999 - accuracy: 0.2129 - val_loss: 2.5079 - val_accuracy: 0.2524\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.5792 - accuracy: 0.2207 - val_loss: 2.4814 - val_accuracy: 0.2600\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 2.5538 - accuracy: 0.2288 - val_loss: 2.4470 - val_accuracy: 0.2694\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 2.5280 - accuracy: 0.2393 - val_loss: 2.4136 - val_accuracy: 0.2903\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.5033 - accuracy: 0.2488 - val_loss: 2.3764 - val_accuracy: 0.2979\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.4745 - accuracy: 0.2507 - val_loss: 2.3413 - val_accuracy: 0.3131\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.4434 - accuracy: 0.2604 - val_loss: 2.3048 - val_accuracy: 0.3188\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.4351 - accuracy: 0.2658 - val_loss: 2.2761 - val_accuracy: 0.3226\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.4002 - accuracy: 0.2758 - val_loss: 2.2447 - val_accuracy: 0.3264\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.3805 - accuracy: 0.2822 - val_loss: 2.2188 - val_accuracy: 0.3378\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.3558 - accuracy: 0.2836 - val_loss: 2.1937 - val_accuracy: 0.3397\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.3368 - accuracy: 0.2953 - val_loss: 2.1723 - val_accuracy: 0.3416\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.3082 - accuracy: 0.2903 - val_loss: 2.1551 - val_accuracy: 0.3416\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2960 - accuracy: 0.2977 - val_loss: 2.1394 - val_accuracy: 0.3416\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2803 - accuracy: 0.2991 - val_loss: 2.1252 - val_accuracy: 0.3416\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2796 - accuracy: 0.3005 - val_loss: 2.1146 - val_accuracy: 0.3454\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2484 - accuracy: 0.3062 - val_loss: 2.0980 - val_accuracy: 0.3454\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2498 - accuracy: 0.3017 - val_loss: 2.0862 - val_accuracy: 0.3491\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2362 - accuracy: 0.3079 - val_loss: 2.0752 - val_accuracy: 0.3510\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.2237 - accuracy: 0.3100 - val_loss: 2.0648 - val_accuracy: 0.3510\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1935 - accuracy: 0.3188 - val_loss: 2.0537 - val_accuracy: 0.3529\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1880 - accuracy: 0.3238 - val_loss: 2.0451 - val_accuracy: 0.3529\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1882 - accuracy: 0.3363 - val_loss: 2.0363 - val_accuracy: 0.3548\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 2.1787 - accuracy: 0.3378 - val_loss: 2.0284 - val_accuracy: 0.3624\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 2.1680 - accuracy: 0.3390 - val_loss: 2.0212 - val_accuracy: 0.3719\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1384 - accuracy: 0.3608 - val_loss: 2.0141 - val_accuracy: 0.3700\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1548 - accuracy: 0.3539 - val_loss: 2.0049 - val_accuracy: 0.3681\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1273 - accuracy: 0.3686 - val_loss: 1.9968 - val_accuracy: 0.4042\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1037 - accuracy: 0.3767 - val_loss: 1.9863 - val_accuracy: 0.3928\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1075 - accuracy: 0.3705 - val_loss: 1.9789 - val_accuracy: 0.3985\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.1003 - accuracy: 0.3734 - val_loss: 1.9703 - val_accuracy: 0.4023\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0945 - accuracy: 0.3786 - val_loss: 1.9624 - val_accuracy: 0.4137\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0680 - accuracy: 0.3914 - val_loss: 1.9527 - val_accuracy: 0.4213\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0931 - accuracy: 0.3760 - val_loss: 1.9458 - val_accuracy: 0.4156\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0769 - accuracy: 0.3788 - val_loss: 1.9391 - val_accuracy: 0.4175\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0535 - accuracy: 0.3916 - val_loss: 1.9325 - val_accuracy: 0.4345\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0596 - accuracy: 0.3990 - val_loss: 1.9263 - val_accuracy: 0.4383\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0571 - accuracy: 0.3888 - val_loss: 1.9182 - val_accuracy: 0.4137\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0245 - accuracy: 0.3964 - val_loss: 1.9097 - val_accuracy: 0.4269\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0294 - accuracy: 0.3959 - val_loss: 1.9033 - val_accuracy: 0.4345\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0219 - accuracy: 0.3988 - val_loss: 1.8968 - val_accuracy: 0.4421\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0211 - accuracy: 0.3990 - val_loss: 1.8901 - val_accuracy: 0.4630\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 2.0075 - accuracy: 0.4106 - val_loss: 1.8822 - val_accuracy: 0.4744\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9896 - accuracy: 0.4180 - val_loss: 1.8759 - val_accuracy: 0.4801\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9969 - accuracy: 0.4054 - val_loss: 1.8688 - val_accuracy: 0.4839\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.9637 - accuracy: 0.4261 - val_loss: 1.8608 - val_accuracy: 0.4744\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.9833 - accuracy: 0.4275 - val_loss: 1.8504 - val_accuracy: 0.4934\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.9613 - accuracy: 0.4239 - val_loss: 1.8420 - val_accuracy: 0.4953\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9336 - accuracy: 0.4313 - val_loss: 1.8316 - val_accuracy: 0.5066\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9481 - accuracy: 0.4327 - val_loss: 1.8219 - val_accuracy: 0.4934\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9338 - accuracy: 0.4386 - val_loss: 1.8135 - val_accuracy: 0.4934\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9089 - accuracy: 0.4375 - val_loss: 1.8005 - val_accuracy: 0.4877\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9169 - accuracy: 0.4479 - val_loss: 1.7888 - val_accuracy: 0.4972\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.9014 - accuracy: 0.4441 - val_loss: 1.7790 - val_accuracy: 0.5104\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8989 - accuracy: 0.4491 - val_loss: 1.7709 - val_accuracy: 0.5028\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8763 - accuracy: 0.4576 - val_loss: 1.7599 - val_accuracy: 0.5199\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8817 - accuracy: 0.4576 - val_loss: 1.7520 - val_accuracy: 0.5142\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8602 - accuracy: 0.4560 - val_loss: 1.7423 - val_accuracy: 0.5199\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8604 - accuracy: 0.4669 - val_loss: 1.7334 - val_accuracy: 0.5351\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8509 - accuracy: 0.4610 - val_loss: 1.7268 - val_accuracy: 0.5275\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8464 - accuracy: 0.4659 - val_loss: 1.7173 - val_accuracy: 0.5161\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8294 - accuracy: 0.4645 - val_loss: 1.7108 - val_accuracy: 0.5275\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8383 - accuracy: 0.4714 - val_loss: 1.7044 - val_accuracy: 0.5199\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8284 - accuracy: 0.4707 - val_loss: 1.6977 - val_accuracy: 0.5085\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8083 - accuracy: 0.4866 - val_loss: 1.6877 - val_accuracy: 0.5180\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.8057 - accuracy: 0.4852 - val_loss: 1.6812 - val_accuracy: 0.5256\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.7863 - accuracy: 0.4811 - val_loss: 1.6729 - val_accuracy: 0.5332\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.7879 - accuracy: 0.4866 - val_loss: 1.6703 - val_accuracy: 0.5332\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.7687 - accuracy: 0.4885 - val_loss: 1.6628 - val_accuracy: 0.5408\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7906 - accuracy: 0.4911 - val_loss: 1.6564 - val_accuracy: 0.5446\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7632 - accuracy: 0.4856 - val_loss: 1.6486 - val_accuracy: 0.5579\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7505 - accuracy: 0.4949 - val_loss: 1.6441 - val_accuracy: 0.5446\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7598 - accuracy: 0.4887 - val_loss: 1.6429 - val_accuracy: 0.5655\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7483 - accuracy: 0.5082 - val_loss: 1.6315 - val_accuracy: 0.5655\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7322 - accuracy: 0.5082 - val_loss: 1.6265 - val_accuracy: 0.5617\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7237 - accuracy: 0.5155 - val_loss: 1.6230 - val_accuracy: 0.5636\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7147 - accuracy: 0.5117 - val_loss: 1.6153 - val_accuracy: 0.5712\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7242 - accuracy: 0.5049 - val_loss: 1.6125 - val_accuracy: 0.5769\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7115 - accuracy: 0.5208 - val_loss: 1.6078 - val_accuracy: 0.5920\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6874 - accuracy: 0.5269 - val_loss: 1.6025 - val_accuracy: 0.5731\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6835 - accuracy: 0.5253 - val_loss: 1.5971 - val_accuracy: 0.5901\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.7077 - accuracy: 0.5212 - val_loss: 1.5940 - val_accuracy: 0.5882\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6888 - accuracy: 0.5269 - val_loss: 1.5926 - val_accuracy: 0.5958\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6795 - accuracy: 0.5217 - val_loss: 1.5887 - val_accuracy: 0.5958\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6522 - accuracy: 0.5348 - val_loss: 1.5849 - val_accuracy: 0.5958\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6539 - accuracy: 0.5229 - val_loss: 1.5795 - val_accuracy: 0.5939\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6520 - accuracy: 0.5400 - val_loss: 1.5735 - val_accuracy: 0.5863\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6517 - accuracy: 0.5341 - val_loss: 1.5700 - val_accuracy: 0.5920\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6264 - accuracy: 0.5390 - val_loss: 1.5666 - val_accuracy: 0.6091\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6530 - accuracy: 0.5333 - val_loss: 1.5627 - val_accuracy: 0.6129\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6379 - accuracy: 0.5414 - val_loss: 1.5581 - val_accuracy: 0.6072\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6286 - accuracy: 0.5488 - val_loss: 1.5549 - val_accuracy: 0.6091\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6097 - accuracy: 0.5500 - val_loss: 1.5489 - val_accuracy: 0.6205\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6255 - accuracy: 0.5526 - val_loss: 1.5446 - val_accuracy: 0.6167\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5966 - accuracy: 0.5497 - val_loss: 1.5414 - val_accuracy: 0.6129\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6340 - accuracy: 0.5419 - val_loss: 1.5370 - val_accuracy: 0.6224\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5915 - accuracy: 0.5576 - val_loss: 1.5328 - val_accuracy: 0.6281\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.6039 - accuracy: 0.5523 - val_loss: 1.5314 - val_accuracy: 0.6319\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5687 - accuracy: 0.5583 - val_loss: 1.5264 - val_accuracy: 0.6338\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5936 - accuracy: 0.5621 - val_loss: 1.5235 - val_accuracy: 0.6376\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5664 - accuracy: 0.5666 - val_loss: 1.5214 - val_accuracy: 0.6395\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5431 - accuracy: 0.5737 - val_loss: 1.5133 - val_accuracy: 0.6376\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5318 - accuracy: 0.5777 - val_loss: 1.5114 - val_accuracy: 0.6395\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5543 - accuracy: 0.5728 - val_loss: 1.5089 - val_accuracy: 0.6433\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5477 - accuracy: 0.5780 - val_loss: 1.5024 - val_accuracy: 0.6433\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5288 - accuracy: 0.5825 - val_loss: 1.5033 - val_accuracy: 0.6471\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5404 - accuracy: 0.5820 - val_loss: 1.4977 - val_accuracy: 0.6509\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5292 - accuracy: 0.5808 - val_loss: 1.4942 - val_accuracy: 0.6528\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5212 - accuracy: 0.5735 - val_loss: 1.4931 - val_accuracy: 0.6395\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.5061 - accuracy: 0.5965 - val_loss: 1.4893 - val_accuracy: 0.6528\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5153 - accuracy: 0.5882 - val_loss: 1.4858 - val_accuracy: 0.6528\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.5207 - accuracy: 0.5777 - val_loss: 1.4862 - val_accuracy: 0.6509\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.5017 - accuracy: 0.5830 - val_loss: 1.4799 - val_accuracy: 0.6490\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4828 - accuracy: 0.5984 - val_loss: 1.4757 - val_accuracy: 0.6452\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4995 - accuracy: 0.5882 - val_loss: 1.4708 - val_accuracy: 0.6528\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4676 - accuracy: 0.6010 - val_loss: 1.4671 - val_accuracy: 0.6565\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4659 - accuracy: 0.5944 - val_loss: 1.4688 - val_accuracy: 0.6528\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4491 - accuracy: 0.6098 - val_loss: 1.4634 - val_accuracy: 0.6546\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4644 - accuracy: 0.6010 - val_loss: 1.4609 - val_accuracy: 0.6528\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4402 - accuracy: 0.6029 - val_loss: 1.4554 - val_accuracy: 0.6528\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4501 - accuracy: 0.6148 - val_loss: 1.4497 - val_accuracy: 0.6641\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4206 - accuracy: 0.6138 - val_loss: 1.4510 - val_accuracy: 0.6546\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4207 - accuracy: 0.6152 - val_loss: 1.4436 - val_accuracy: 0.6509\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4426 - accuracy: 0.6065 - val_loss: 1.4429 - val_accuracy: 0.6565\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4206 - accuracy: 0.6141 - val_loss: 1.4378 - val_accuracy: 0.6565\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.4180 - accuracy: 0.6143 - val_loss: 1.4359 - val_accuracy: 0.6584\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3885 - accuracy: 0.6186 - val_loss: 1.4298 - val_accuracy: 0.6565\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3941 - accuracy: 0.6100 - val_loss: 1.4264 - val_accuracy: 0.6603\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3947 - accuracy: 0.6195 - val_loss: 1.4246 - val_accuracy: 0.6584\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.3891 - accuracy: 0.6226 - val_loss: 1.4184 - val_accuracy: 0.6603\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.3747 - accuracy: 0.6226 - val_loss: 1.4151 - val_accuracy: 0.6603\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.3590 - accuracy: 0.6235 - val_loss: 1.4100 - val_accuracy: 0.6641\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3535 - accuracy: 0.6321 - val_loss: 1.4096 - val_accuracy: 0.6641\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3475 - accuracy: 0.6226 - val_loss: 1.4051 - val_accuracy: 0.6679\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3449 - accuracy: 0.6364 - val_loss: 1.4022 - val_accuracy: 0.6603\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3454 - accuracy: 0.6321 - val_loss: 1.4018 - val_accuracy: 0.6660\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3139 - accuracy: 0.6454 - val_loss: 1.3925 - val_accuracy: 0.6660\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3347 - accuracy: 0.6373 - val_loss: 1.3934 - val_accuracy: 0.6622\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3338 - accuracy: 0.6278 - val_loss: 1.3929 - val_accuracy: 0.6660\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3256 - accuracy: 0.6366 - val_loss: 1.3933 - val_accuracy: 0.6584\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3013 - accuracy: 0.6482 - val_loss: 1.3839 - val_accuracy: 0.6679\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3077 - accuracy: 0.6409 - val_loss: 1.3827 - val_accuracy: 0.6660\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.3162 - accuracy: 0.6385 - val_loss: 1.3813 - val_accuracy: 0.6679\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2768 - accuracy: 0.6556 - val_loss: 1.3780 - val_accuracy: 0.6641\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2801 - accuracy: 0.6504 - val_loss: 1.3739 - val_accuracy: 0.6679\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2724 - accuracy: 0.6459 - val_loss: 1.3655 - val_accuracy: 0.6679\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2610 - accuracy: 0.6537 - val_loss: 1.3688 - val_accuracy: 0.6641\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2730 - accuracy: 0.6535 - val_loss: 1.3604 - val_accuracy: 0.6698\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2769 - accuracy: 0.6504 - val_loss: 1.3604 - val_accuracy: 0.6717\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2397 - accuracy: 0.6565 - val_loss: 1.3551 - val_accuracy: 0.6717\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2703 - accuracy: 0.6563 - val_loss: 1.3552 - val_accuracy: 0.6698\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2616 - accuracy: 0.6421 - val_loss: 1.3504 - val_accuracy: 0.6736\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.2463 - accuracy: 0.6561 - val_loss: 1.3501 - val_accuracy: 0.6717\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2169 - accuracy: 0.6613 - val_loss: 1.3471 - val_accuracy: 0.6774\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.2179 - accuracy: 0.6660 - val_loss: 1.3405 - val_accuracy: 0.6793\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2146 - accuracy: 0.6667 - val_loss: 1.3394 - val_accuracy: 0.6774\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2128 - accuracy: 0.6589 - val_loss: 1.3393 - val_accuracy: 0.6774\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2040 - accuracy: 0.6641 - val_loss: 1.3424 - val_accuracy: 0.6774\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1960 - accuracy: 0.6684 - val_loss: 1.3332 - val_accuracy: 0.6774\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.2195 - accuracy: 0.6629 - val_loss: 1.3369 - val_accuracy: 0.6774\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1817 - accuracy: 0.6739 - val_loss: 1.3351 - val_accuracy: 0.6812\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1747 - accuracy: 0.6739 - val_loss: 1.3287 - val_accuracy: 0.6831\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1960 - accuracy: 0.6601 - val_loss: 1.3297 - val_accuracy: 0.6831\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1748 - accuracy: 0.6770 - val_loss: 1.3255 - val_accuracy: 0.6793\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1591 - accuracy: 0.6724 - val_loss: 1.3222 - val_accuracy: 0.6831\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1785 - accuracy: 0.6696 - val_loss: 1.3204 - val_accuracy: 0.6831\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1469 - accuracy: 0.6741 - val_loss: 1.3175 - val_accuracy: 0.6850\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1369 - accuracy: 0.6853 - val_loss: 1.3178 - val_accuracy: 0.6793\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1345 - accuracy: 0.6826 - val_loss: 1.3149 - val_accuracy: 0.6812\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1451 - accuracy: 0.6784 - val_loss: 1.3113 - val_accuracy: 0.6869\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1333 - accuracy: 0.6789 - val_loss: 1.3116 - val_accuracy: 0.6793\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1115 - accuracy: 0.6876 - val_loss: 1.3030 - val_accuracy: 0.6869\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1220 - accuracy: 0.6836 - val_loss: 1.3068 - val_accuracy: 0.6869\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1251 - accuracy: 0.6874 - val_loss: 1.3060 - val_accuracy: 0.6812\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 1.1140 - accuracy: 0.6971 - val_loss: 1.3050 - val_accuracy: 0.6812\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 1.1062 - accuracy: 0.6962 - val_loss: 1.3042 - val_accuracy: 0.6812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "8Jd0HBg6FaVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing loss and accuracy of the model on the test set\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FksY9EwwHDkd",
        "outputId": "b1426478-e6d7-4889-c8e7-13d93f12ae9e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2315 - accuracy: 0.7078\n",
            "Loss:  1.2314647436141968\n",
            "Accuracy:  0.7077798843383789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the history of training and its keys\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPdyDruxHIv1",
        "outputId": "e39a0db1-91a0-4af4-dc22-b7dddc28fb50"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)"
      ],
      "metadata": {
        "id": "shbF_xRlHDqZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Results"
      ],
      "metadata": {
        "id": "C6VBFUZnFpXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting of loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iLsoTgHGP7Oq",
        "outputId": "da0d7c01-234e-42b5-f470-158601614c3c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting of accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1uWqq-CtP-Lq",
        "outputId": "58c3e3b4-9907-4b63-9798-f2964f372f42"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "cjOG8y0PF9Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the probability model for testing\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "\n",
        "# predicting test samples\n",
        "predictions = probability_model.predict(raw_test_batch.map(vectorize_text))"
      ],
      "metadata": {
        "id": "YQj61lIMvX-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969d906d-c198-4a6e-bac7-2ffd46657c73"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the first test sample result label\n",
        "np.argmax(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbhBKR7NYPQ",
        "outputId": "c5582f9c-4c9a-44e6-d5d5-f4e67c0b796d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the true label of the first test sample\n",
        "test_df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNsnTWVuu3xj",
        "outputId": "cd4aa15e-405a-4153-dc48-9d5c6786a757"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         starter pack curly or dry hair nuebar argan oi...\n",
              "label                                          Health & Beauty\n",
              "label_int                                                   10\n",
              "Name: 2991, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}